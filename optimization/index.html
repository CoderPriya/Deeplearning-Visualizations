<!DOCTYPE html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn how to appropriately optimize your neural networks.">
    <meta name="author" content="--">
    <title>Optimization</title>
    <!-- Fonts -->
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="https://fonts.googleapis.com/css?family=Assistant:300,400,600,700" rel="stylesheet">
    <!-- Home Page CSS -->
    <link rel="stylesheet" type="text/css" href="../css/template.css">
    <!--Favicon-->
    <link rel="shortcut icon" type="image/png" href="../img/favicon.png" />
    <!-- Load jquery -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <!-- Load D3 -->
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <!--Tool Tip-->
    <script src="../js/d3.tip.js"></script>
    <!-- Load Tensorflow -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.9.0">
    </script>
    <!-- Load Katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>
    <!--Scroll To-->
    <script src="../js/template.js"></script>
</head>

<body>
    <div class="header"> 
        <div class="header-wrapper">

        	<a href="https://www.deeplearning.ai/"><img id="header-logo" class="cppnControl" src="../img/deeplearning.png"></a>
            <a><img id="header-logo" class="cppnControl"></a>
            <!-- Uncomment below to add links to other articles -->
            <ul class="header-nav">
                 <li><a href="../Regularization/index.html">Regularization</a></li>
                <li><strong>Optimization</strong></li>
                <li><a href="../Initialization/index.html">Initialization</a></li>
            </ul>
        </div>
    </div>
    <div class="main vis-background">
        <div class="container" >
            <div class="column-6-8 column-align" >
                <h1 class="title">Optimizing your neural networks</h1>
            </div>
        </div>
        
    </div>
    <div class="main intro ">
        <div class="container divider-bottom" >
            <div class="column-6-8 column-align" >
               
                <h2 class="title">Machine learning is an optimization problem. Various optimization techniques exist, and different optimizers lead to different trained models. Through interactive visualizations, you will learn the intuition behind (I) setting-up your optimization problem and (II) running the neural network optimization process.</h2>
               
            </div>

            <div class="column-2-8 column-align">
                <h3 class="tableOfContent">Table of content</h3>
                <ol class="tableOfContent" type="I">
                    <li class="index index1">Setting-up the optimization problem</li>
                    <li class="index index2">Running the optimization process</li>
                </ol>
            </div>
        </div>
    </div>
    <div class="main">
        <div class="container index1-target">
            <div class="column-6-8 column-align">
            	
            	<p>In machine learning, you first define a task, such as detecting cats on an image, and a loss (or objective) function to evaluate how well you perform on that task. Then, you find the architecture and parameters that result in an optimal value for your objective.</p>

                <h3>I&emsp;Setting-up the optimization problem</h3>
                <p>The way you define your task is the primary factor influencing how your model is going to train and perform. Say your task is to localize and identify cars in images. Your objective function should frame the following sentence in mathematical terms:</p>
                <p class="inline-caption">Given an image, output bounding boxes (bbox) that contain all the cars on the image. Your output boxes should match the labeled boxes.</p>
                <p>In mathematical terms, a possible loss function <script>
                    document.write(katex.renderToString('\\mathcal{L}'))
                </script> is:</p>

                <!-- YOLO LOSS FUNCTION LATEX FORMULA (see below, should be per line though) + explanation of terms 
				-->
                <!-- In the margin for this term: This term ensures that the center of the predicted bounding box should match the center of the ground thruth box.
                -->
                <!-- In the margin for this term: This term ensures that the width (resp. height) of the predicted bounding box should match the width (resp. height) of the ground thruth box.
                -->
                <!-- In the margin for this term: In object detection, there's usually a probability of objectness. This term ensures that the probability of objectness of the predicted bounding box should match the probability of objectness of the ground thruth box.
                -->
                <!-- In the margin for this term: In object detection, there's usually a probability of objectness. This term ensures that the probability of objectness of the predicted bounding box should match the probability of objectness of the ground thruth box if there is no ground thruth bounding box for the specific cell (i, j) in the YOLO output volume.
                -->
                <!-- In the margin for this term: This term ensures that the class of the predicted bounding box should match the class of the ground thruth box.
                -->

                <script>
                var render = katex.renderToString("\\begin{aligned}\\mathcal{L} &= \\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\left[ (x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2 \\right] &\\text{BBox Center}\\cr &+ \\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\left[ (\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2 \\right] &\\text{BBox Width/Height}\\cr &+ \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} (C_i - \\hat{C}_i)^2 &\\text{Objectness}\\cr &+\\lambda_{noobj}\\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} (C_i - \\hat{C}_i)^2 &\\text{No Objectness}\\cr &+\\sum_{i=0}^{S^2} \\mathbb{1}_{i}^{obj} \\sum_{c \\in classes} (p_i(c) - \\hat{p}_i(c))^2 &\\text{Class Probabilities}\\end{aligned}", { displayMode: true });
                document.writeln(render);
                </script>
                

                <p>This <span class="marginanchor" id="margin-0-anchor" data-number="0" data-align="top">loss function</span> depends on:</p>
                <ul>
                    <li>The parameters of the network (weights and biases)</li>
                    <li>The batch of inputs given to the network</li>
                    <li>The batch of ground truths corresponding to the input batch given to the network</li>
                </ul>
                <p>For a fixed value of the input batch and ground truths batch, the loss has a landscape that depends on the parameters of the network. It is almost impossible to visualize the loss landscape (against the parameters) if there are more than 2 parameters. However, the landscape does exist and our goal is to find the point where the loss value is minimum. Updating the values of the parameters will move the loss value either closer to or farther to the target minimum point. In another AI note, you will learn techniques to visualize your loss function in higher dimensions.</p>
            </div>
            <div class="column-2-8 column-align margin">
                <div class="marginbody" id="margin-0-body" data-number="0">
                    <p class="caption">If this function L has a low value, then the found parameters define a network function performing well on the task.</p>
                </div>
            </div>
            <div class="column-6-8">
                <h4>The duality between the network function and the loss function</h4>
            </div>
            <div class="column-6-8 column-align">
                <p>In optimizing your machine learning algorithm, it is important to note the difference between the function <script>document.write(katex.renderToString('\\mathcal{L}'))</script> you are optimizing (i.e. loss function) and the function <script>document.write(katex.renderToString('f'))</script> you will use to perform the task (i.e. the network function).</p>
                <ul>
                    <li><b>The network function</b> is unknown. Its purpose is to mimic the <span class="marginanchor" id="margin-1-anchor" data-number="1" data-align="middle">real function</span> that performs the task. You know that the network function is defined by its parameters, and your goal is to find them.</li>
                    <li><b>The loss function</b> is chosen for a specific task, and you have a formula for it. But its value depends on the parameters and thus the network function.</li>
                </ul>
                <p>Interestingly (or counterintuitively), you are not optimizing the function that you will use to predict your end value (network function). Instead, you are defining a “proxy” function (loss function) which, if minimized, will make your network function correctly.</p>
                <p>You need a loss function because the network function is unknown and you are not sure how to find the correct network function.</p>
                <p>In summary, the way you define your optimization problem will dictate the performance of your network function.</p>
                <img src="img/functiongraph.png">
            </div>
            <div class="column-2-8 column-align margin">
                <div class="marginbody" id="margin-1-body" data-number="1">
                    <p class="caption">Do you know the mathematical formula that allows you to detect cats on images? Probably not, but using data you can find a function that performs this task. It turns out that a convolutional architecture with the right parameters define a function that would perform this task well.</p>
                </div>
            </div>
        </div>
        <div class="container index2-target">
            <div class="column-6-8 column-align">
                <h3>II&emsp;Running the optimization process</h3>
                <p>In this section, assume that you have already chosen a task and a loss function. You will minimize the loss function to find the network function.</p>
            </div>
            <div class="column-6-8">
                <h4>Initialization, learning rate and batch size</h4>
            </div>
            <div class="column-6-8 column-align">
                <p>First, initialize the parameters of the network function so that you have a starting point for your optimization. You also need to choose your hyperparameters such as the learning rate and the batch size. They will have an important influence on the optimization.</p>
                <p>In the visualization below, you will play with the starting point of initialization, the learning rate, and the batch size. With these parameters, you will fit a linear regression on a set of 300 data points using the gradient descent optimization algorithm. For more information on gradient descent optimization, refer to the Deep Learning Specialization (<a href="https://www.coursera.org/learn/neural-networks-deep-learning">Course 1</a>: “Neural Networks and Deep Learning”, Week 2: “Logistic Regression as a Neural Network”.) Here are some questions you should ask yourself:</p>
                <ul>
                    <li>Why does the loss landscape look like this?</li>
                    <li>What is the red bullet?</li>
                    <li>Why can your optimization end-up with a lower cost value than the ground truth line?</li>
                    <li>What is the impact of the learning rate on the optimization?</li>
                    <li>What is the impact of the batch size on the optimization?</li>
                </ul>
            </div>
        </div>
        <div class="full-container" id="landscape">
            <div class="viz-column-2-8">
                <h3>1. Generate your dataset</h3>
                <p>Choose the slope and intercept for the <span id="" class="bold">"ground truth" function</span> that you will try to learn. Click on generate to draw random points from a normal distribution centered on the "ground truth" function.</p>
                <!-- <button class="button-transport" id="sample" >
                	Sample Training Set
            	</button> -->
            	<div>
            		Select training set size:<br>
                    <label class="radio-container">Small
	                    <input type="radio" value="20" name="regression_tsize">
	                    <span class="checkmark"></span>
	                </label>
	                <label class="radio-container">Appropriate
	                    <input type="radio" value="300" name="regression_tsize" checked>
	                    <span class="checkmark"></span>
	                </label>
	                <label class="radio-container">Large
	                    <input type="radio" value="800" name="regression_tsize">
	                    <span class="checkmark"></span>
	                </label>
            	</div>
                <svg id="regression_plot" width=300 height=300></svg>
                <button class="button-transport" id="generate" >
                	new "ground truth" function
            	</button>
            </div>
            <div class="viz-column-4-8">
                <h3>2. Observe the loss landscape and initialize parameters</h3>
                <p>Sample a training set and explore the loss landscape. 
                 The blue dot indicates the value of the loss function for an optimal slope and intercept.
                 The red dot indicates the value of the loss function for a chosen initialization of the slope and intercept. Feel free to drag the red dot and choose an initialization for your slope and intercept.</p>
                <svg id="regression_landscape" width=450 height=450></svg>
            </div>
            <div class="viz-column-2-8">
                <h3>3. Search Hyperparameters</h3>
                
                <div>
                    Select the learning rate to use:<br>
                    <label class="radio-container">Small
	                    <input type="radio" value="0.0001" name="regression_lrate">
	                    <span class="checkmark"></span>
	                </label>
	                <label class="radio-container">Appropriate
	                    <input type="radio" value="0.001" name="regression_lrate" checked>
	                    <span class="checkmark"></span>
	                </label>
	                <label class="radio-container">Large
	                    <input type="radio" value="0.01" name="regression_lrate">
	                    <span class="checkmark"></span>
	                </label>
	                <br>
                    Select the batch size to use:<br>
                    <label class="radio-container">Stochastic
	                    <input type="radio" value="1" name="regression_bsize" checked>
	                    <span class="checkmark"></span>
	                </label>
	                <label class="radio-container">Mini
	                    <input type="radio" value="30" name="regression_bsize">
	                    <span class="checkmark"></span>
	                </label>
	                <label class="radio-container">Full
	                    <input type="radio" value="300" name="regression_bsize">
	                    <span class="checkmark"></span>
	                </label>
                </div>
                <button class="button-transport" id="regression_reset" title="reset">
                    <img src="img/reset.png">
                </button>
                <button class="button-transport" id="regression_train" title="start">
                    <img src="img/play.png">
                </button>
                <button class="button-transport hidden" id="regression_stop" title="stop">
                    <img src="img/pause.png">
                </button>
                <svg id="regression_loss" width=300 height=200></svg>
            </div>
        </div>
        <div class="container">
            <div class="column-6-8 column-align">
                <p>Here are some insights you can take away from the visualization above:</p>
            </div>
            <div class="column-6-8">
                <h4>Initialization</h4>
                <p>A good initialization can make your optimization quicker and converge to the correct minimum (in the case where there are several minima.) If you want to learn more about initialization, read our AI Note on <a href="../Initialization/index.html">“Initializing your neural network”</a>.</p>
            </div>
            <div class="column-6-8 column-align">
                <h4>Learning rate</h4>
                <p>The choice of learning rate influences the convergence of your optimization.</p>
                <ul>
                    <li>If the learning rate is too small, your updates are small and the optimization is slow. Furthermore, you’re likely to settle into an <span class="marginanchor" id="margin-3-anchor" data-number="3" data-align="middle">inappropriate local minimum</span>.</li>
                    <li>If the learning rate is too large, your updates will be large and the optimization is likely to diverge.</li>
                    <li>If the learning rate is appropriate, your updates are appropriate and the optimization will converge quickly.</li>
                </ul>
                <p>The choice of learning rate depends on the curvature of your loss function. Gradient descent makes a linear approximation of the loss function in a given point. It then moves downhill along the approximation of the loss function. In the case where the loss is highly curved, the larger your step size (learning rate) the larger the error of your approximation. Taking small steps reduces your error<sup class="footnote-index footnote-index1">1</sup>.</p>
                <img src="img/losscurve.png">
            </div>
            <div class="column-2-8 column-align margin">
                <div class="marginbody" id="margin-3-body" data-number="3">
                    <p class="caption">Here we say inappropriate local minimum because in machine learning optimization, it is unlikely to converge to the global minimum.</p>
                </div>
            </div>
            <div class="column-6-8">
                <h4>Batch size</h4>
                <p>The right choice of batch size is crucial to ensure convergence and generalization of your network. There’s no consensus yet on what batch size to choose. Here’s an example of a situation.</p>
                <p>Here are the suggested heuristics to use when choosing your batch sizes:</p>
                <ul>
                    <li>Choosing the largest batch size that fits in your GPU memory will usually accelerate your training.</li>
                    <li>However, larger batch sizes can also hurt the ability to generalize. There’s a trade-off to find depending on the task you’re trying to achieve.</li>
                    <li>The larger the batch size, the more accurate the gradient of the loss will be with respect to the parameters, i.e. the direction of the update is most likely going down the local slope of the loss landscape.</li>
                </ul>
                <p>In practice, to find hyperparameters such as the learning rate and the batch size, we perform hyperparameter search.</p>
            </div>
            <div class="column-6-8">
                <h4>Iterative update</h4>
            </div>
            <div class="column-6-8">
                <p>Now that you have a starting point, a learning rate, and a batch size, it’s time to iteratively update the parameters to move towards the minimum of the loss function. The optimization algorithm is also a core choice to be made. There are various optimizers, and you can play with them in the visualization below. It will help you build intuition on the pros and cons of each of them.</p>
            </div>
        </div>
        <div class="full-container" id="landscape">
            <div class="viz-column-3-8">
                <h3>1. Choose your loss landscape</h3>
                <p>Select an <a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization">artificial landscape</a> <script>document.write(katex.renderToString('f(x,y)'))</script>.</p>
                <div class="lossFunctions">
                    <label>
                      <input type="radio" name="loss" value="goldsteinPrice"/>
                      <img src="./img/loss/goldsteinPrice.png">
                    </label>
                    <label>
                      <input type="radio" name="loss" value="himmelblaus" checked/>
                      <img src="./img/loss/himmelblaus.png">
                    </label>
                    <label>
                      <input type="radio" name="loss" value="rosenbrock" />
                      <img src="./img/loss/rosenbrock.png">
                    </label>
                    <label>
                      <input type="radio" name="loss" value="styblinskiTang" />
                      <img src="./img/loss/styblinskiTang.png">
                    </label>
                </div>
                </br>
                <h3>2. Choose your initial parameters</h3>
                <p>Drag the red dot to choose the initial parameters values and thus the initial value of the loss.</p>

                <h3>3. Choose your optimizer</h3>
                <p>Select the optimizers to use and their hyperparameters.</p>

                <table>
                  <tr>
                    <th>Optimizer</th>
                    <th>Learning Rate</th> 
                    <th>Learning Rate Decay</th>
                  </tr>
                  <tr>
                    <td><input type="checkbox" name="opt" value="gd"/> Gradient Descent</td>
                    <td><input class="gd" type="number" name="lrate" value="0.001" min="0" max="1" step="0.0001"/></td>
                    <td><input class="gd" type="number" name="ldecay" value="0" min="0" max="1" step="0.01"/></td>
                  </tr>
                  <tr>
                    <td><input type="checkbox" name="opt" value="momentum"/> Momentum</td>
                    <td><input class="momentum" type="number" name="lrate" value="0.001" min="0" max="1" step="0.0001"/></td>
                    <td><input class="momentum" type="number" name="ldecay" value="0" min="0" max="1" step="0.01"/></td>
                  </tr>
                  <tr>
                    <td><input type="checkbox" name="opt" value="rmsprop"/> RMSprop</td>
                    <td><input class="rmsprop" type="number" name="lrate" value="0.001" min="0" max="1" step="0.0001"/></td>
                    <td><input class="rmsprop" type="number" name="ldecay" value="0" min="0" max="1" step="0.01"/></td>
                  </tr>
                  <tr>
                    <td><input type="checkbox" name="opt" value="adam"/> Adam</td>
                    <td><input class="adam" type="number" name="lrate" value="0.001" min="0" max="1" step="0.0001"/></td>
                    <td><input class="adam" type="number" name="ldecay" value="0" min="0" max="1" step="0.01"/></td>
                  </tr>
                </table>
                </br>
                <button class="button-transport" id="reset" title="reset">
                    <img src="img/reset.png">
                </button>
                <button class="button-transport" id="train" title="start">
                    <img src="img/play.png">
                </button>
                <button class="button-transport hidden" id="stop" title="stop">
                    <img src="img/pause.png">
                </button>
            </div>
            <div class="viz-column-4-8">
                <h3>Loss Landscape: <script>document.write(katex.renderToString('(x_{t+1},y_{t+1}) = (x_{t},y_{t}) + \\delta^{t}\\alpha g(x,y)'))</script></h3>
                <svg id="contour" width="550" height="520"></svg>
                <h3>Loss Plot: <script>document.write(katex.renderToString('L(x,y) = f(x,y) + \\lambda r(x,y)'))</script></h3>
                <svg id="loss" width="550" height="200"></svg>
            </div>
        </div>
        <div class="container">
            <div class="column-6-8">
                <p>The choice of optimizer influences both the speed and the occurrence of your convergence. A lot of alternatives to the classic gradient descent algorithms have been developed in the past few years and are listed in the table below. (Notation: <script>
                    document.write(katex.renderToString('dW = \\frac{\\partial \\mathcal{L}}{\\partial W}'))
                    </script>)</p>
                <table>
                  <tr>
                    <th>Optimizer</th>
                    <th>Update rule</th> 
                    <th>Attribute</th>
                  </tr>
                  <tr>
                    <td>(Stochastic) Gradient Descent</td>
                    <td>
                        <script>document.write(katex.renderToString('W = W - \\alpha dW'))</script>
                    </td>
                    <td>
                        <ul>
                            <li><small>GD is very slow when the dataset is large, given that GPUs can’t handle the whole data in memory and thus parallelization isn’t optimal.</small></li>
                            <li><small>SGD usually converges faster than GD on large datasets, because updates are more frequent. Plus, the stochastic approximation of the gradient is usually precise without using the whole dataset because the data is often redundant.</small></li>
                            <li><small>GD can use parallelization efficiently.</small></li>
                        </ul>
                    </td>
                  </tr>
                  <tr>
                    <td>Momentum</td>
                    <td>
                        <script>
                            document.write(katex.renderToString('V_{dW} = \\beta V_{dW} + ( 1 - \\beta ) dW'))
                        </script>
                        <script>
                            document.write(katex.renderToString('W = W - \\alpha V_{dW}'))
                        </script>
                     </td>
                    <td>    
                        <ul>
                            <li><small>Momentum boosts to speed of learning with a very minor implementation change.</small></li>
                        </ul>
                    </td>
                  </tr>
                  <tr>
                    <td>RMSprop</td>
                    <td>
                        <script>
                        document.write(katex.renderToString('S_{dW} = \\beta S_{dW} + ( 1 - \\beta ) dW^2'))
                        </script>
                        <script>
                        document.write(katex.renderToString('W = W - \\alpha \\frac{dW}{\\sqrt{S_{dW}} + \\varepsilon}'))
                        </script>
                    </td>
                    <td>
                        <ul>
                            <li><small>RMSprop’s adaptive learning rate prevents the learning rate decay from diminishing too slowly or too fast.</small></li>
                            <li><small>RMSprop maintains per-parameter learning rates.</small></li>
                            <li><small>RMSprop usually works well in online and non-stationary settings.</small></li>
                        </ul>
                    </td>
                  </tr>
                  <tr>
                    <td>Adam</td>
                    <td>
                        <script>
                        document.write(katex.renderToString('V_{dW} = \\beta_1 V_{dW} + ( 1 - \\beta_1 ) dW'))
                        </script>
                        <script>
                        document.write(katex.renderToString('S_{dW} = \\beta_2 S_{dW} + ( 1 - \\beta_2 ) dW^2'))
                        </script>
                        <script>
                        document.write(katex.renderToString('Vcorr_{dW} = \\frac{V_{dW}}{(1 - \\beta_1)^t}'))
                        </script> 
                        <script>
                        document.write(katex.renderToString('Scorr_{dW} = \\frac{S_{dW}}{(1 - \\beta_2)^t}'))
                        </script> 
                        <script>
                        document.write(katex.renderToString('W = W - \\alpha \\frac{dW}{\\sqrt{S_{dW}} + \\varepsilon}'))
                        </script>    
                     </td>
                    <td>
                        <ul>
                            <li><small>The hyperparameters of Adam (learning rate, Exponential decay rates for the moment estimates, etc.) are usually set to predefined values (given in the paper), and do not require to be tuned.</small></li>
                            <li><small>Adam performs a form of learning rate annealing with adaptive step-sizes.</small></li>
                            <li><small>Adam is often the default optimizer in machine learning.</small></li>
                        </ul>
                    </td>
                  </tr>
                </table>
                <p>For more information about these optimizers, refer to the Deep Learning Specialization (<a href="https://www.coursera.org/learn/deep-neural-network">Course 2</a>: “Improving your Deep Neural Network”, Week 2: “Optimization”).</p>
                <p>Exploring the optimization methods and hyperparameters presented above helps you build intuition that you can use to optimize networks for your own tasks. Intuitively understanding  the sensitivity of the loss optimization for these hyperparameters (learning rate, batch size, optimizer, etc.) is very important during hyperparameter search. Combined with the right method (random search or bayesian optimization), it will help you iterate through your search in order to find the right model.</p>
            </div>
        </div>
    </div>
    <div class="footer">
        <div class="container">
            <div class="column-2-8 column-align">
                <h4 class="reference">Authors</h4>
            </div>
            <div class="column-6-8 column-align">
                <ol class="reference ">
                    <li><a href="https://twitter.com/kiankatan">Kian Katanforoosh</a> - Written content and structure. </li>
                    <li><a href="http://daniel-kunin.com">Daniel Kunin</a> - Visualizations (created using <a href="https://d3js.org/">D3.js</a> and <a href="https://js.tensorflow.org/">TensorFlow.js</a>).</li>
                </ol>
            </div>
            <div class="column-2-8 column-align">
                <h4 class="reference">Acknowledgments</h4>
            </div>
            <div class="column-6-8 column-align">
                <ol class="reference">
                    <li>The template for the article was designed by <a href="https://www.jingru-guo.com/">Jingru Guo</a> and inspired by <a href="https://distill.pub/">Distill</a>.</li>
                    <li>The loss landscape visualization adapted code from Mike Bostock's <a href="https://bl.ocks.org/mbostock/f48ff9c1af4d637c9a518727f5fdfef5">visualization</a> of the Goldstein-Price function.</li>
                    <li>The banner visualization adapted code from deeplearn.js's implementation of a <a href="https://deeplearnjs.org/demos/nn-art/">CPPN</a>.</li>
                </ol>
            </div>
            <div class="column-2-8 column-align">
                <h4 class="reference">Footnotes</h4>
            </div>
            <div class="column-6-8 column-align">
                <ol class="reference footnote">
                    <li class="footnote-index1-target">Chapter 4 of the Deep Learning textbook (numerical computation) from Goodfellow et al.</li>
                </ol>
            </div>
            <div class="column-2-8 column-align">
                <h4 class="reference">Reference</h4>
            </div>
            <div class="column-6-8 column-align">
                <p class="reference">To reference this article in an academic context, please cite this work as:</p>
                <p class="citation">Katanforoosh & Kunin, "Optimizing your neural networks", deeplearning.ai, 2018.</p>
            </div>
        </div>
    </div>
    <div class="backToTop">
        <p>↑ Back to top</p>
    </div>
</body>

<!-- Additional JS -->
<script src="https://d3js.org/d3-contour.v1.min.js"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/d3-legend/2.25.6/d3-legend.min.js"></script>

<!-- LANDSCAPE -->
<link rel="stylesheet" href="css/landscape.css">
<script src="js/landscape/point.js"></script>
<script src="js/landscape/loss.js"></script>
<script src="js/landscape/optimizer.js"></script>
<script src="js/landscape/viz.js"></script>

<!-- REGRESSION -->
<link rel="stylesheet" href="css/regression.css">
<script src="js/regression/line.js"></script>
<script src="js/regression/loss.js"></script>
<script src="js/regression/optimizer.js"></script>
<script src="js/regression/viz.js"></script>

<!-- CPPN -->
<script src="js/cppn.js"></script>
