<!DOCTYPE html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn how to appropriately initialize parameters in your neural networks.">
    <meta name="author" content="--">
    <title>Regularization</title>
    <!-- Fonts -->
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="https://fonts.googleapis.com/css?family=Assistant:300,400,600,700" rel="stylesheet">
    <!-- Home Page CSS -->
    <link rel="stylesheet" type="text/css" href="../css/template.css">
    <!--Favicon-->
    <link rel="shortcut icon" type="image/png" href="../img/favicon.png" />
    <!-- Load jquery -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <!-- Load D3 -->
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <!-- Load Tensorflow -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.9.0">
    </script>
    <!-- Load Katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>
    <!--Scroll To-->
    <script src="../js/template.js"></script>
</head>

<body>
    <div class="header"> 
        <div class="header-wrapper">

        	<a href="https://www.deeplearning.ai/"><img id="header-logo" class="cppnControl" src="../img/deeplearning.png"></a>
            <a><img id="header-logo" class="cppnControl"></a>
            <ul class="header-nav">
                <li><strong>Regularization</strong></li>
                <li><a href="../Optimization/index.html">Optimization</a></li>
                <li><a href="../Initialization/index.html">Initialization</a></li>
            </ul>
        </div>
    </div>
    <div class="main vis-background">
        <div class="container" >
            <div class="column-6-8 column-align" >
                <h1 class="title">Regularizing your neural networks</h1>
                 <p class="title" > <a href="https://twitter.com/kiankatan">Kian Katanforoosh</a> & <a href="http://daniel-kunin.com">Daniel Kunin</a></p>
            </div>
        </div>
        
    </div>
    <div class="main intro ">
        <div class="container divider-bottom" >
            <div class="column-6-8 column-align" >
               
                <h2 class="title">Regularization methods are designed to help your model generalize to unseen data, rather than overfitting the training data. In this post, you will learn various methods we use in order to regularize our models. Our recommendations will be accompanied by intuitive explanations.</h2>
               
            </div>

            <div class="column-2-8 column-align">
                <h3 class="tableOfContent">Table of content</h3>
                <ol class="tableOfContent" type="I">
                    <li class="index index1">The importance of regularization</li>
                    <li class="index index2">Early stopping</li>
                    <li class="index index3">L1 and L2 regularizations</li>
                    <li class="index index4">Dropout regularization</li>
                    <!-- <li class="index index5">Methodology to train generalizable models</li> -->
                </ol>
            </div>
        </div>
    </div>

    <div class="main">
        <div class="container index1-target">
            <div class="column-6-8 column-align">
                <h3>I&emsp;The importance of regularization</h3>
                <p>The ultimate goal for your neural network is to generalize to “unseen data”, in order to be used in the real world. The ability of a neural network to generalize to unseen data depends on two factors:
                </p>
                <ul>
                    <li>The information in the training data. A dataset of images taken from the front-facing camera of a car contains, for example, much more information than a dataset of images of clouds in the sky.</li>
                    <li>The complexity of the network. By complexity, we mean how complex is the function your neural network can mimic. Usually, the more parameters your network uses, the more complex it is.</li>
                </ul>
            </div>
            <div class="column-6-8">
                <h3>Case 1: the network is not complex enough, and the training data contains a lot of information.</h3>
            </div>
            <div class="column-6-8 column-align">
                <p>Your network is too simple to understand the training data’s salient features. This is called <span class="marginanchor" id="margin-0-anchor" data-number="0" data-align="middle">underfitting</span> the training set.</p>
                <p class="inline-caption"><b>Example:</b> You train a 2-layer convolutional neural network on an ImageNet classification task (1000 classes, that are very different from each other.)</p>
                <p>A common trick to avoid underfitting is to deepen your neural network by adding more layers.</p>
            </div>
            <div class="column-2-8 column-align margin">
            	<div class="marginbody" id="margin-0-body" data-number="0">
                    <img src="img/underfit.png">
                	<p class="caption">As shown above the network underfit the training data</p>
                </div>
            </div>
            <div class="column-6-8">
                <h3>Case 2: the network is very complex, but the training data doesn’t contain too much information.</h3>
            </div>
            <div class="column-6-8 column-align">
                <p>Your network is <span class="marginanchor" id="margin-1-anchor" data-number="1" data-align="middle">complex</span> enough to fully memorize the mapping between the training data and the training labels. It won’t generalize because it didn’t need to understand the salient features of the dataset to perform well on the training set. This is called <span class="marginanchor" id="margin-2-anchor" data-number="2" data-align="middle">overfitting</span> the training set.</p>
                <p class="inline-caption"><b>Example:</b> You train a 20-layer convolutional neural network to detect if there’s a cloud (label 1) or no clouds (label 0) in the sky.</p>
                <p>The best way to make your model generalize is to gather a larger dataset, but this is not always possible. Regularization methods are designed to help your model generalize, and not overfit the training data. Let’s learn the methods we use in order to regularize our models, and the intuition behind it.</p>
            </div>
            <div class="column-2-8 column-align margin">
                <div class="marginbody" id="margin-1-body" data-number="1">
                    <p class="caption">The number of parameter (weights) is usually a proxy to measure the complexity of a network.</p>
                </div>
                <div class="marginbody" id="margin-2-body" data-number="2">
                    <img src="img/overfit.png">
                    <p class="caption">You train a 20-layer convolutional neural network to detect if there’s a cloud (label 1) or no clouds (label 0) in the sky.</p>
                </div>
            </div>
            <div class="column-6-8">
                <h3>Data Split: partioning a data set.</h3>
            </div>
            <div class="column-6-8 divider-bottom column-align">
                <p>In order to estimate the ability of your model to generalize, you will split your dataset into three (or sometimes more) sets: <span class="marginanchor" id="margin-3-anchor" data-number="3" data-align="middle">training</span>, <span class="marginanchor" id="margin-4-anchor" data-number="4" data-align="middle">validation (dev)</span> and <span class="marginanchor" id="margin-5-anchor" data-number="5" data-align="middle">test</span>. Your model is able to generalize if it was trained on the training set, tuned on the validation set, and still performs well on the test set.</p>
                <table>
                  <tr>
                    <th>Train Accuracy</th>
                    <th>Dev Accuracy</th> 
                    <th>Test Accuracy</th>
                    <th>Conclusion</th>
                  </tr>
                  <tr>
                    <td>99%</td>
                    <td>75%</td>
                    <td>70%</td>
                    <td>Overfitting (high variance)</td>
                  </tr>
                  <tr>
                    <td>88%</td>
                    <td>85%</td>
                    <td>83%</td>
                    <td>Underfitting (high bias)</td>
                  </tr>
                  <tr>
                    <td>95%</td>
                    <td>93%</td>
                    <td>92%</td>
                    <td>Appropriate (correct bias/variance trade-off)</td>
                  </tr>
                </table>
                <p>Let’s delve into the methods to help get the test set performance closer to the dev and train set performances as in the third row of the table above.</p>
            </div>
            <div class="column-2-8 column-align margin">
                <div class="marginbody" id="margin-3-body" data-number="3">
                    <p class="caption">The data used to train your model.</p>
                </div>
                <div class="marginbody" id="margin-4-body" data-number="4">
                    <p class="caption">The data used to tune the hyperparameters of your model.</p>
                </div>
                <div class="marginbody" id="margin-5-body" data-number="5">
                    <p class="caption">The data used to evaluate the performance of your model in real life.</p>
                </div>
            </div>
        </div>
        <div class="container">
            <div class="column-6-8 index2-target column-align">
                <h3>II&emsp; Early stopping</h3>
                <p>The easiest but widely used method is called early stopping. During the iterative <span class="marginanchor" id="margin-6-anchor" data-number="6" data-align="middle">optimization</span> process of finding the correct parameters for your model, if you evaluate your model’s error on the training and validation set, you might see such curves:</p>
                <img src="img/curves.png">
                <p>Based on this observation, you can state that after the 30,000th <span class="marginanchor" id="margin-7-anchor" data-number="7" data-align="middle">epoch</span>, your model starts overfitting to the training set. Early stopping means “saving the model’s parameters at the 30,000th epoch”. The saved model is the best performing model on the dev set and will likely generalize better to the test set.</p>
                <p class="inline-caption"><b>Example:</b> Consider a “day vs. night image classification”. Rather than understanding the <span class="marginanchor" id="margin-8-anchor" data-number="8" data-align="middle">inherent features</span> of the data such as the brightness, the colors or the presence of a blue sky, your model learned mapping between training images and training labels by heart. You do not want this.</p>
            </div>
            <div class="column-2-8 column-align margin">
                <div class="marginbody" id="margin-6-body" data-number="6">
                    <p class="caption">Examples of optimizations are Stochastic Gradient Descent, Adam, Momentum or RMS Prop.</p>
                </div>
                <div class="marginbody" id="margin-7-body" data-number="7">
                    <p class="caption">An epoch is one optimization pass over the entire dataset. In full-batch gradient descent, one epoch is one iteration, but in stochastic gradient descent, one epoch is multiple iterations.</p>
                </div>
                <div class="marginbody" id="margin-8-body" data-number="8">
                    <p class="caption">What are the hints telling us that the image was taken during the day or the night?</p>
                </div>
            </div>
            <div class="column-6-8">
                <h3>What are the main advantages of early stopping?</h3>
            </div>
            <div class="column-6-8 divider-bottom column-align">
                <ul>
                    <li>It is easy to do. Deep learning frameworks, such as Keras, offer options to save your model’s parameters regularly during training. Here is the corresponding lines of code and <a href="https://keras.io/callbacks/#earlystopping" target="_blank">documentation</a> on Keras to stop training when a chosen quantity has stopped improving.</li>
                    <p class="citation">keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None)</p>
                    <li>It is quicker than other regularization methods, because you do not have any regularization hyperparameter to tune. As a comparison, L2 regularization requires you to tune a regularization hyperparameter. It might take you several experiments to regularize your model with L2, but only one run to do so with early stopping.</li>
                </ul>
            </div>
        </div>

        <div class="container">
            <div class="column-6-8 index3-target">
                <h3>III&emsp; L1 and L2 regularizations</h3>
                <p>In order to avoid overfitting the training set, one can try to reduce the complexity of the model by removing layers, and consequently decreasing the number of parameters. As shown by the work of <a href="https://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization.pdf" target="_blank">Krogh and Hertz (1992)</a>, another way to constrain a network and lower its complexity is to:</p>
                <p class="inline-caption">Limit the growth of the weights through some kind of weight decay.</p>
                <p>The goal is to prevent the weights from growing too large, unless it is really necessary.</p>
            </div>
            <div class="column-4-8">
                <h3>How does this work?</h3>
            </div>
            <div class="column-6-8 divider-bottom column-align">
                <p>L1 and L2 regularizations can be achieved by simply adding a term that penalizes large weights to the cost function. If you were training a network to minimize the cost <script> document.write(katex.renderToString('J_{cross-entropy} = - \\frac{1}{m}\\sum_{i=1}^m y^{(i)}log(\\hat{y}^{(i)})'))</script> with (let’s say) gradient descent, your weight update rule would be:</p>
                <script>
	                var render = katex.renderToString("w = w - \\alpha \\frac{\\partial J_{cross-entropy}}{\\partial w}", { displayMode: true });
	                document.writeln(render);
                </script>
                <p>Instead, you would now train a network to minimize the cost <script> document.write(katex.renderToString('J_{regularized} = J_{cross-entropy} + \\lambda J_{\\text{L1 or L2}}'))</script> where <script> document.write(katex.renderToString('J_{L1} = \\sum_{\\text{all weights } w_k} |w_k|'))</script> and <script> document.write(katex.renderToString('J_{L2} = \\sum_{\\text{all weights } w_k} ||w_k||_2^2'))</script>. Your weight update rule would be:</p>
                <script>
	                var render = katex.renderToString("w = w - \\alpha \\frac{\\partial J_{regularized}}{\\partial w} = w - \\alpha ( \\frac{\\partial J_{cross-entropy}}{\\partial w} + \\lambda \\frac{\\partial J_{L1 or L2}}{\\partial w} )", { displayMode: true });
	                document.writeln(render);
                </script>
                <p>For <span class="marginanchor" id="margin-4-anchor" data-number="4" data-align="middle">L1 regularization</span>, this would lead to the update rule:</p>
            	<script>
	                var render = katex.renderToString("w = w - \\underbrace{\\alpha \\lambda sign(w)}_\\text{L1 penalty} - \\underbrace{\\alpha \\frac{\\partial J_{cross-entropy}}{\\partial w}}_\\text{Gradient penalty}", { displayMode: true });
	                document.writeln(render);
                </script>
                <p>For <span class="marginanchor" id="margin-3-anchor" data-number="3" data-align="middle">L2 regularization</span>, this would lead to the update rule:</p>
                <script>
                    var render = katex.renderToString("w = w - \\underbrace{\\alpha \\lambda w}_\\text{L2 penalty} - \\underbrace{\\alpha \\frac{\\partial J_{cross-entropy}}{\\partial w}}_\\text{Gradient penalty}", { displayMode: true });
                    document.writeln(render);
                </script>
                <p>As you can see, with L1 and L2 regularization, at every step, the weight is pushed to a slightly lower value because <script> document.write(katex.renderToString('\\alpha \\lambda << 1'))</script>, causing “weight decay”.</p>
            </div>
            <div class="column-2-8 column-align margin">
                <div class="marginbody" id="margin-4-body" data-number="4">
                    <p class="caption">In statistics, L1 regularization is sometimes refered to as Lasso regression.</p>
                </div>
                <div class="marginbody" id="margin-3-body" data-number="3">
                    <p class="caption">In statistics, L2 regularization is sometimes refered to as ridge regression.</p>
                </div>
            </div>
            <div class="column-6-8">
                <h3>What’s the difference between L1 and L2?</h3>
            </div>
            <div class="column-6-8 column-align">
                <p>The update rules are different. While the L2 “weight decay” <script> document.write(katex.renderToString('\\alpha\\lambda w'))</script> penalty is proportional to the value of the weight to be updated, the L1 “weight decay” <script> document.write(katex.renderToString('\\alpha\\lambda sign(w)'))</script> is not.</p>
                <p>For L2, it means the smaller <script> document.write(katex.renderToString('w'))</script>, the smaller the penalty during the update of <script> document.write(katex.renderToString('w'))</script> and vice-versa for larger <script> document.write(katex.renderToString('w'))</script>.</p>
                <p>For L1, the penalty is independent on the value of <script> document.write(katex.renderToString('w'))</script>, but the direction of the penalty (positive or negative) depends on the sign of <script> document.write(katex.renderToString('w'))</script>. This results in an effect called “feature selection” or “weight sparsity”.</p>
                <p>Let’s play with the visualization below to see the impact of L1 and L2 regularization on the weights during training.</p>
            </div>
        </div>

        <div class="full-container" id="mnist">
            <div class="viz-column-4-8">
                <div id="mnist_network"></div>
            </div>
            <div class="viz-column-3-8">
                <p>Use the following selections to view a histogram of the weight values during training of a network <span id="regularized" class="bold">with</span> and <span id="unregularized" class="bold">without</span> regularization.</p>
            	<h3>1. Select a regularization method</h3>
            	<label class="radio-container">L1 regularization
                    <input type="radio" value="1" name="mnist_init" checked>
                    <span class="checkmark"></span>
                </label>
                <label class="radio-container">L2 regularization
                    <input type="radio" value="0" name="mnist_init">
                    <span class="checkmark"></span>
                </label>
                <h3>2. Choose a value for the regularization constant</h3>
                <label class="radio-container">Too small
                    <input type="radio" value="0.0001" name="lambda_sparsity">
                    <span class="checkmark"></span>
                </label>
                <label class="radio-container">Appropriate
                    <input type="radio" value="0.01" name="lambda_sparsity" checked>
                    <span class="checkmark"></span>
                </label>
                <label class="radio-container">Too large
                    <input type="radio" value="1" name="lambda_sparsity">
                    <span class="checkmark"></span>
                </label>
                <h3>3. Load your dataset and train the network</h3>
                <p>Train a <span id="regularized" class="bold">regularized</span> and <span id="unregularized" class="bold">unregularized</span> neural network and construct a histogram of the weight values during training.</p>
                <div>
                	<button class="button emphasized" id="mnist_load">
                        Load Data (<span id="percent">0%</span>)
                    </button>
                    <button class="button-transport" id="mnist_reset"><img src="img/reset.png"></button>
                    <button class="button-transport inactive" id="mnist_start"><img src="img/play.png"></button>
                    <button class="button-transport hidden" id="mnist_stop"><img src="img/pause.png"></button>
                    <button class="button-transport inactive" id="mnist_step"><img src="img/fastforward.png"></button>
                </div>
                </br>
                <label class="viz">
                    Batch: <span id="batch">0</span>
                </label>
                <label class="viz">
                    Epoch: <span id="epoch">0</span>
                </label>
            </div>
            
        </div>
        <div class="container">
	        <div class="column-6-8">
	            <p>As you can see, L1 and L2 regularizations have a dramatic effect on the weights values:</p>
	            <ul>
                    <li>For the L2 regularization: the weight values decrease following a normal distribution that becomes more and more peaked throughout training.</li>
                    <li>For the L1 regularization: many of the weights become null. This is called “sparsity of the weights”. Because the weight penalty is independent of the weight values, weights with value 0.001 are penalized as much as weights with value 1000. The value of the penalty is <script> document.write(katex.renderToString('\\alpha\\lambda'))</script> (generally very small), and constrains a subset weights that are “less useful to the model” to be equal to 0. For example, you could effectively end up with 200 non-zero weights out of 1,000, meaning that 800 weights were “less useful to learn the task”.</li>
                    <li>The weight sparsity effect resulting of L1 regularization makes your model more compact in theory, and leads to storage efficient compact models that are commonly used in smart mobile devices.</li>
                </ul>
	        </div>
            <div class="column-4-8">
                <h3>How to implement L1 and L2 regularization?</h3>
            </div>
            <div class="column-6-8">
	            <p>Deep learning frameworks such as Keras allow you to add L1 or L2 regularization to your network in one line of code. The difference in the optimization process is implemented automatically, here’s an example: <a href="https://keras.io/regularizers/" target="_blank">L1 and L2 regularization in Keras</a>.</p>
	        </div>
	        <div class="column-6-8">
                <h3>How does this help the model generalize?</h3>
            </div>
	        <div class="column-6-8">
	            <ul>
                    <li>Weight decay suppresses any irrelevant components of the weight vector by driving the optimization to find the smallest vector that solves the learning problem.</li>
                    <li>It attenuates the influence of outliers on the weight optimization, by constraining the weight values. There’s less risk for the weights to learn the sampling error (for example when a subset of your data points comes from a wrong distribution, or is mislabelled.) In other words, the output of the model is less affected by changes in the input.</li>
                </ul>
                <p>In order to build some intuition about what L1 and L2 regularization do. Let’s play with the visualization below to observe variations of the cost landscape subject to regularization from a top view.</p>
	        </div>
        </div>
        <div class="full-container" id="landscape">
            <div class="viz-column-4-8">
                <svg id="contour" width="550" height="520"></svg>
            </div>
            <div class="viz-column-3-8" style="padding-left: 5%;">
                <h3>1. Choose an <a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization" target="_blank">artificial</a> loss landscape</h3>
                <p></p>
                <div class="lossFunctions">
                    <label>
                      <input type="radio" name="loss" value="beale" />
                      <img src="./img/loss/beale.png">
                    </label>
                    <label>
                      <input type="radio" name="loss" value="goldsteinPrice"/>
                      <img src="./img/loss/goldsteinPrice.png">
                    </label>
                    <label>
                      <input type="radio" name="loss" value="himmelblaus" checked/>
                      <img src="./img/loss/himmelblaus.png">
                    </label>
                    <label>
                      <input type="radio" name="loss" value="matyas" />
                      <img src="./img/loss/matyas.png">
                    </label>
                    <label>
                      <input type="radio" name="loss" value="mcCormick" />
                      <img src="./img/loss/mcCormick.png">
                    </label>
                    <label>
                      <input type="radio" name="loss" value="rastrigin" />
                      <img src="./img/loss/rastrigin.png">
                    </label>
                    <label>
                      <input type="radio" name="loss" value="rosenbrock" />
                      <img src="./img/loss/rosenbrock.png">
                    </label>
                    <label>
                      <input type="radio" name="loss" value="styblinskiTang" />
                      <img src="./img/loss/styblinskiTang.png">
                    </label>
                </div>
                </br>
                <h3>2. Select a regularization method</h3>
                </br>
                <label class="radio-container">L1 regularization
                    <input type="radio" value="1" name="reg">
                    <span class="checkmark"></span>
                </label>
                <label class="radio-container">L2 regularization
                    <input type="radio" value="0" name="reg" checked>
                    <span class="checkmark"></span>
                </label>
                </br></br>
	            <h3>3. Choose a value for the regularization constant</h3>
                </br>
                <input type="range" value="0" min="0" max="40" step="any" id="lambda">
                </br></br>
                <label>Lambda = <span id="lambda_val">0.00</span></label>
            </div>
        </div>
        <div class="container">
	        <div class="column-6-8">
	            <p>As you can see, L1 and L2 regularizations have a dramatic effect on the geometry of the cost function:</p>
	            <ul>
                    <li>Using the 6th landscape (Rastrigin Function), notice that adding regularization results in a more convex cost landscape and diminishes the chance of falling into a non-desired minimum.</li>
                    <li>Using the 3rd landscape (Himmelblaus Function), notice that adding L2 regularization prevents the network to converge in a local minimum.</li>
                </ul>
	        </div>
        </div>

        <div class="container">
            <div class="column-6-8 divider-bottom index4-target">
                <h3>IV&emsp; Dropout regularization</h3>
                <p>If you had unlimited computational power, you could improve generalization by averaging the predictions of several different neural networks trained on the same task. The combination of these models will likely perform better than a single neural network trained on this task. However, with the deep neural networks, training various architectures is expensive because:</p>
             	<ol>
             		<li>Tuning the hyperparameters is time consuming.</li>
             		<li>Training the networks requires a lot of computations.</li>
             		<li>You need a large amount of data to train the models on different subsets of the data.</li>
             	</ol>
                <p>Dropout is a regularization technique that allows you to combine many different architectures efficiently by randomly dropping some of the neurons of your network during training.</p>
            	<!-- <img src="img/dropout.png"> -->
            	<video width="70%" height="auto" controls>
					<source src="img/dropout1.mp4" type="video/mp4">
					Your browser does not support the video tag.
				</video>
            	<p>Dropout is discussed in details in our <a href="https://www.coursera.org/specializations/deep-learning" target="_blank">Deep Learning Specialization</a> (<a href="https://www.coursera.org/learn/deep-neural-network" target="_blank">Course 2: “Improving Neural Networks”</a>, Week 2: “Practical aspects of deep learning”) and we invite you to check it out to understand the intuition behind its usage. </p>
            </div>
        </div>

        <div class="container">
            <div class="column-6-8 index5-target">
                <!-- <h3>V&emsp; Our methodology to train generalizable models at deeplearning.ai</h3>
                <p>Here’s a concrete example of how we would tackle a deep learning task and try to make our model generalize.</p>
                <p>Given a complex speech (audio) dataset to perform trigger word detection, this is roughly how we would find the best model:</p>
                <ul>
                    <li>Find a model that is able to overfit a single training example: the model can perfectly detect the trigger word in the training audio clip<sup class="footnote-index footnote-index1">1</sup>.</li>
                    <li>Tweak the previous model to be able to overfit a larger pool of training example: the model can perfectly detect the trigger words in the large pool of training audio clips.</li>
                    <li>Overfit the entire training dataset (or the maximum number of training examples you can overfit).</li>
                    <li>Use the regularization techniques discussed above to make your model generalize to unseen data: the model can detect the trigger word for voices and backgrounds that are not in the training dataset<sup class="footnote-index footnote-index2">2</sup>.</li>
                </ul> -->
                <p>Successfully training a model on complex tasks is complicated. You need to find the appropriate model architecture able to understand the complexity of the dataset. Once you found such an architecture, make it generalize using regularization.</p>
            </div>
        </div>

    </div>
        
    <div class="footer">
        <div class="container">
            <div class="column-2-8 column-align">
                <h4 class="reference">Authors</h4>
            </div>
            <div class="column-6-8 column-align">
                <ol class="reference ">
                    <li><a href="https://twitter.com/kiankatan">Kian Katanforoosh</a> - Written content and structure. </li>
                    <li><a href="http://daniel-kunin.com">Daniel Kunin</a> - Visualizations (created using <a href="https://d3js.org/">D3.js</a> and <a href="https://js.tensorflow.org/">TensorFlow.js</a>).</li>
                </ol>
            </div>
            <div class="column-2-8 column-align">
                <h4 class="reference">Acknowledgments</h4>
            </div>
            <div class="column-6-8 column-align">
                <ol class="reference">
                    <li>The template for the article was designed by <a href="https://www.jingru-guo.com/">Jingru Guo</a> and inspired by <a href="https://distill.pub/">Distill</a>.</li>
                    <li>The loss landscape visualization adapted code from Mike Bostock's <a href="https://bl.ocks.org/mbostock/f48ff9c1af4d637c9a518727f5fdfef5">visualization</a> of the Goldstein-Price function.</li>
                    <li>The banner visualization adapted code from deeplearn.js's implementation of a <a href="https://deeplearnjs.org/demos/nn-art/">CPPN</a>.</li>
                </ol>
            </div>
            <div class="column-6-8 column-align">
                <h4 class="reference">Footnotes</h4>
                <ol class="reference footnote">
                	<!-- <li class="footnote-index1-target">Intuitively, neural networks mimic functions. But not all functions can map an audio clip to the correct sequence of labels. You need to search extensively for an architecture that understands the underlying mapping between an audio clip and the sequence of labels. You might think that a very deep model with high capacity (i.e. a lot of parameters) should always be able to overfit a single example. It is true, but deep models are super hard to train, and you may struggle a lot to overfit even a single training example.</li>
                	<li class="footnote-index2-target">You will need to collect large validation (dev) and test sets in order to track the ability of your model to generalize.</li> -->
                </ol>
            </div>
            <div class="column-6-8 column-align">
                <ol class="reference footnote">
                    <li class="footnote-index1-target"><a href="https://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization.pdf">A Simple Weight Decay Can Improve Generalization</a></li>
                    <li class="footnote-index2-target"><a href="https://www.coursera.org/learn/neural-networks/lecture/n6TUy/the-bayesian-interpretation-of-weight-decay-11-min">The Bayesian interpretation of weight decay</a></li>
                    <li class="footnote-index3-target"><a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
                </ol>
            </div>
            <div class="column-2-8 column-align">
                <h4 class="reference">Reference</h4>
            </div>
            <div class="column-6-8 column-align">
                <p class="reference">To reference this article in an academic context, please cite this work as:</p>
                <p class="citation">Katanforoosh & Kunin, "Regularizing your neural networks", deeplearning.ai, 2018.</p>
                <!-- <p class="reference">BibTeX citation:</p>
                <p class="citation">@article{kiank+dkunin,<br>
                  &emsp;author = {Katanforoosh, Kian and Kunin, Daniel},<br>
                  &emsp;title = {Initializing your neural networks},<br>
                  &emsp;journal = {deeplearning.ai},<br>
                  &emsp;year = {2018}}</p> -->
            </div>
        </div>
    </div>
    <div class="backToTop">
        <p>↑ Back to top</p>
    </div>
</body>

<!-- LANDSCAPE -->
<script src="https://d3js.org/d3-contour.v1.min.js"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/d3-legend/2.25.6/d3-legend.min.js"></script>
<link rel="stylesheet" href="css/landscape.css">
<script src="js/landscape/loss.js"></script>
<script src="js/landscape/viz.js"></script>

<!-- SPARSITY -->
<link rel="stylesheet" href="css/sparsity.css">
<script src="js/sparsity/nn.js"></script>
<script src="js/sparsity/zip/zip.js"></script>
<script src="js/sparsity/data.js"></script>
<script src="js/sparsity/viz.js"></script>

<!-- CPPN -->
<script src="js/cppn.js"></script>
