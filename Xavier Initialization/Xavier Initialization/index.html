<!DOCTYPE html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="A visual introduction to probability and statistics.">
    <meta name="author" content="--">
    <title>Xavier Initialization</title>
    <!-- Fonts -->
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="https://fonts.googleapis.com/css?family=Assistant:300,400,600,700" rel="stylesheet">
    <!-- Home Page CSS -->
    <link rel="stylesheet" type="text/css" href="css/template.css">
    <!--Favicon-->
    <link rel="shortcut icon" type="image/png" href="img/favicon.png" />
    <!-- Load jquery -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <!-- Load D3 -->
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <!-- Load Deeplearn -->
    <script src="https://unpkg.com/deeplearn@latest"></script>
    <!-- Load Katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>

</head>

<body>
    <div class="header"></div>
    <div class="main divider-bottom intro">
        <div class="container ">
            <div class="column-6-8 column-align">
                <h1 class="title">Xavier Initialization</h1>
                <h2>The lack of initialization techniques is one of the major reasons deep neural networks weren’t training properly in the past decades. Since then, new initializations schemes have been found to accelerate training. In this post, we will explain the methods to initialize the parameters of your neural network.</h2>
            </div>
            <div class="column-2-8 column-align">
                <h3 class="tableOfContent">Table of content</h3>
                <ul class="tableOfContent">
                    <li class="index index1">I&emsp; Problem statement</li>
                    <li class="index index2">II&emsp; What are the issues with a bad initialization?</li>
                    <li class="index index3">III&emsp; How do we fix this issue?</li>
                    <li class="index index4">IV&emsp;Derivation of Xavier initialization with Tanh</li>
                </ul>
            </div>
        </div>
    </div>
    <div class="main">
        <div class="container index1-target">
            <div class="column-6-8">
                <h3>I&emsp;Problem statement</h3>
                <p>A neural network model is defined by its <i>architecture</i> and its <i>parameters</i>. In order to build a cat classifier with a 3-layer neural network (architecture), you need to learn the parameters using the optimization process:
                </p>
                <ol>
                    <li>Initialize the parameters</li>
                    <li>Use an optimization algorithm, repeat:</li>
                    <ol>
                        <li>Forward propagate an input image</li>
                        <li>Compute the cost</li>
                        <li>Backward propagation the gradients</li>
                        <li>Update the parameters using gradient descent (for example)</li>
                    </ol>
                </ol>
                <p>
                Given a new image, you can then use your model (architecture + learned parameters) to predict if there’s a cat.
                </p>
                <p>
                To illustrate our argumentation let’s consider this 3-layer neural network. You can play with different initialization scheme and observe their impact on the learning.
                </p>
            </div>
            
        </div>
        <div class="full-container">
            <div class="viz-column-2-8 ">
                <h3>1. Input Data</h3>
                <p>Choose a dataset as input to the neural network and select whether to view weights or gradients.</p>
                <label class="radio-container">Gradients
                    <input type="radio" value="gradient" name="playground_link">
                    <span class="checkmark"></span>
                </label>
                <label class="radio-container">Weights
                    <input type="radio" value="weight" name="playground_link" checked>
                    <span class="checkmark"></span>
                </label>
        
            </div>
            <div class="viz-column-4-8 ">
                <h3>2. Neural Network</h3>
                <p>Select a variance to initialize weights with.</p>
                <label class="radio-container">Zero
                    <input type="radio" value=0  name="playground_init">
                    <span class="checkmark"></span>
                </label>
                <label class="radio-container">Too Small
                    <input type="radio" value=0.01 name="playground_init">
                    <span class="checkmark"></span>
                </label>
                <label class="radio-container">Appropriate
                    <input type="radio" value=1 name="playground_init" checked>
                    <span class="checkmark"></span>
                </label>
                <label class="radio-container">Too Large
                    <input type="radio" value=100 name="playground_init">
                    <span class="checkmark"></span>
                </label>
               
            </div>
            <div class="viz-column-2-8 ">
                <h3>3. Output Classification</h3>
                <p>Train model.</p>
                <div>
                    <button class="button-transport" id="playground_reset"><img src="img/reset.png"></button>
                    <button class="button-transport" id="playground_start"><img src="img/play.png"></button>
                    <button class="button-transport hidden" id="playground_stop"><img src="img/pause.png"></button>
                    <button class="button-transport" id="playground_step"><img src="img/fastforward.png"></button>
                </div>
            </div>
            <div class="viz-column-2-8 ">
                <div id="playground_dataset"></div>
                <div id="playground_legend"></div>
            </div>
            <div class="viz-column-4-8 ">
                 <div id="playground_network"></div>
            </div>
            <div class="viz-column-2-8 ">
                 <div id="playground_loss"></div>
                <div id="playground_pred"></div>
            </div>
        </div>
        <div class="container">
            <div class="column-6-8 column-align">
                <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
            </div>
            <div class="column-2-8 column-align">
                <p class="caption">Zero initialization doesn’t work because it lacks at breaking the symmetry.</p>
            </div>
            <div class="column-8-8 divider-bottom"></div>
            <div class="column-6-8 index2-target">
                <h3>II&emsp; What are the issues with a bad initialization?</h3>
                <p>An inappropriate initialization leads to:</p>
                <ol>
                    <li>Vanishing gradient -> parameters values get stuck</li>
                    <li>Exploding gradients -> parameters values diverge</li>
                </ol>
                <p>In both cases, parameters do not converge to “good” values. Let’s illustrate this.</p>
                <p>Consider this 9-layer neural network just after it was initialized.</p>
                <img src="img/9layer.png">
                <p>At every iteration of the optimization loop (forward, cost, backward, update), we observe that back-propagated gradients are either amplified or minified as we move from the output layer towards the input layer. This makes sense if you recall the following: Assuming all the activation functions are linear (identity function), the output activation is:</p>
                <script>
                    var render = katex.renderToString("\\hat{y} = a^{[L]} = W^{[L]}W^{[L-1]}W^{[L-2]}\\dots W^{[3]}W^{[2]}W^{[1]}X", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>where <script>document.write(katex.renderToString('L=10'))</script> and <script>document.write(katex.renderToString('W^{[1]},W^{[2]},\\dots,W^{[L-1]}'))</script> are all matrices of size <script>document.write(katex.renderToString('(2,2)'))</script>.</p>
            </div>
        </div>
        <div class="container">
            <div class="column-6-8 divider-bottom"></div>
            <div class="column-4-8 ">
                <h3>Exploding Gradient</h3></div>
            <div class="column-6-8 column-align">
                <p>Consider the case where every weight is initialized slightly larger than the identity matrix.</p> 
                <script>
                    var render = katex.renderToString("W^{[1]} = W^{[2]} = \\dots = W^{[L-1]}=\\begin{bmatrix}1.5 & 0 \\cr 0 & 1.5\\end{bmatrix}", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>This simplifies to <script>document.write(katex.renderToString('\\hat{y} = W^{[L]}1.5^{L-1}X'))</script>, and the values of the activation <script>document.write(katex.renderToString('a^{[l]}'))</script> increase exponentially with <script>document.write(katex.renderToString('l'))</script>. When these activations are used in backward propagation, this leads to the exploding gradient problem.</p>
            </div>
            <div class="column-2-8 column-align">
                <img src="img/exploding.png">
                <p class="caption">This can be visualized on the contour plot of the loss as...</p>
            </div>
            <div class="column-4-8">
                <h3>Vanishing Gradient</h3>
            </div>

            <div class="column-6-8 column-align">
                <p>Similarly, consider the case where every weight is initialized slightly smaller than the identity matrix.</p>
                <script>
                    var render = katex.renderToString("W^{[1]} = W^{[2]} = \\dots = W^{[L-1]}=\\begin{bmatrix}0.5 & 0 \\cr 0 & 0.5\\end{bmatrix}", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>This simplifies to <script>document.write(katex.renderToString('\\hat{y} = W^{[L]}0.5^{L-1}X'))</script>, and the values of the activation <script>document.write(katex.renderToString('a^{[l]}'))</script> decreases exponentially with <script>document.write(katex.renderToString('l'))</script>. When these activations are used in backward propagation, this leads to the vanishing gradient problem.</p>
            </div>
            <div class="column-2-8 column-align">
                <img src="img/vanishing.png">
                <p class="caption">This can be visualized on the contour plot of the loss as...</p>
            </div>
            <div class="column-6-8 divider-bottom">
                <p>All in all, initializing your weights with inappropriate values will lead to divergence or slow-down in the training.</p>
            </div>
            <div class="column-6-8 index3-target">
                <h3>III&emsp;How do we fix this issue?</h3>
                <p>Let's define our rule of thumb to translate the fact that we want the weights to be initialized neither “too big”, nor “too small”:</p>
                <p class="inline-caption">The variance should stay the same across every layer to prevent the signal from vanishing or exploding</p>
                <p>More concretely, consider a layer <script>document.write(katex.renderToString('l'))</script> and its forward propagation:</p>
                <script>
                    var render = katex.renderToString("\\begin{aligned}a^{[l-1]} &= g^{[l-1]}(z^{[l-1]})\\cr z^{[l]} &= W^{[l]}a^{[l-1]} + b^{[l]}\\cr a^{[l]} &= g^{[l]}(z^{[l]}\\end{aligned}", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>We would like the following to hold for all layers:</p>
                <script>
                    var render = katex.renderToString("Var(a^{[l-1]}) = Var(a^{[l]})", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>In the visualization below, observe the impact of the weights’ distribution on each layer’s activations’ distribution.</p>
            </div>
        </div>
        <div class="full-container">
            <div class="viz-column-2-8">
                <h3>1. MNIST</h3>
                <p>Load <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> test dataset (10,000 images).</p>
                <div>
                    <button class="button emphasized" id="mnist_load">
                        Load MNIST (<span id="percent">0%</span>)
                    </button>
                </div>
            </div>
            <div class="viz-column-4-8">
                <h3>2. Hidden Layers</h3>
                <p>Select a distribution to initialize weights with.</p>
                <label class="radio-container">Zero
                    <input type="radio" value="zero" name="mnist_init">
                    <span class="checkmark"></span>
                </label>
                <label class="radio-container">Uniform
                    <input type="radio" value="uniform" name="mnist_init">
                    <span class="checkmark"></span>
                </label>
                <label class="radio-container">Xavier
                    <input type="radio" value="xe" name="mnist_init" checked>
                    <span class="checkmark"></span>
                </label>
                <label class="radio-container">Normal
                    <input type="radio" value="normal" name="mnist_init">
                    <span class="checkmark"></span>
                </label>

            </div>
            <div class="viz-column-2-8">
                <h3>3. Softmax</h3>
                <p><span class="correct bold">Blue</span> squares represent correctly classified images.  <span class="incorrect bold">Red</span> squares represent incorrectly classified images.</p>
                <div>
                    <button class="button-transport" id="mnist_reset"><img src="img/reset.png"></button>
                    <button class="button-transport inactive" id="mnist_start"><img src="img/play.png"></button>
                    <button class="button-transport hidden" id="mnist_stop"><img src="img/pause.png"></button>
                    <button class="button-transport inactive" id="mnist_step"><img src="img/fastforward.png"></button>
                </div>
            </div>
            <div class="viz-column-2-8 ">
                    <p>Input Layer</p>
                    <div id="mnist_input"></div>
                    <label class="viz">
                    Batch: <span id="batch">0</span> </label>
                    <label >
                    Epoch: <span id="epoch">0</span>
                    </label>
            </div>
            <div class="viz-column-4-8 ">
                <div id="mnist_network"></div>
            </div>
            <div class="viz-column-2-8 ">
                <div>
                    <p>Output Layer</p>
                    <div id="mnist_output"></div>
                    <label class="viz">
                    Error: <span id="accuracy">0/100</span>
                    </label>
                    <label>
                    Loss: <span id="cost">0.00</span>
                    </label>
                </div>
            </div>
              <div class="viz-column-8-8 ">
                <img src="img/img4.png">
            </div>
            
        </div>
        <div class="container">
            <div class="column-6-8 column-align">
                <p>More precisely, Glorot et al. (2010) recommended to initialize your network’s parameters with:</p>
                <script>
                    var render = katex.renderToString("\\begin{aligned}W^{[l]} &\\sim Normal(\\mu=0,\\sigma^2 = \\frac{1}{n^{[l-1]}})\\cr b^{[l]} &\\sim 0\\end{aligned}", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>This scheme is called Xavier initialization, and it’s the “appropriate initialization” in the visual above.</p>
            </div>
            <div class="column-2-8 column-align">
                <p class="caption">For more information check out<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi"> Glorot et al. (2010) paper which introduced Xavier initialization.</a></p>
                
            </div>
            <div class="column-6-8 column-align index4-target">
                <h3>IV&emsp;Derivation of Xavier initialization with Tanh</h3>
                <p>In this section, we will go over the math step by step. I think it sometimes help to understand the proof to grasp the concept, but you can get most of the intuition without the math, feel free to jump to the next section.</p>
                <p>Let’s work on the layer <script>document.write(katex.renderToString('l'))</script> described in “How do we fix this issue” and assume the activation functions <script>document.write(katex.renderToString('g^{[l-1]}'))</script> and <script>document.write(katex.renderToString('g^{[l]}'))</script> are <script>document.write(katex.renderToString('tanh'))</script>:</p>
            </div>
            <div class="column-2-8 column-align">
                <img src="img/tanh.png">
                <p class="caption">tanh...</p>
            </div>
            <div class="column-6-8">
                <p>Important properties of <script>document.write(katex.renderToString('tanh'))</script> are its parity (<script>document.write(katex.renderToString('tanh(-x) = -tanh(x)'))</script>) and its linearity around 0 (<script>document.write(katex.renderToString("tanh'(0) = 1"))</script>).</p>
                <p>Let’s work on the right-hand side of (*). Using the fact that <script>document.write(katex.renderToString("g^{[l]} = tanh"))</script> and <script>document.write(katex.renderToString("tanh(z^{[l]})[?][?][?]z^{[l]}"))</script>. We can simplify the right-hand side of (*) to:</p>
                <script>
                    var render = katex.renderToString("Var(a^{[l]}) = Var(tanh(z^{[l]})) = Var(z^{[l]})", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>Moreover, <script>document.write(katex.renderToString("z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} = vector(z_1^{[l]},z_2^{[l]},\\dots,z_{n^{[l]}}^{[l]})"))</script> where <script>document.write(katex.renderToString("z_k^{[l]} = \\sum_{j=1}^{n^{[l-1]}}w_{kj}^{[l]}a_j^{[l-1]} + b_k^{[l]}"))</script>.  For simplicity, let’s assume that <script>document.write(katex.renderToString("b^{[l]} = 0"))</script> (it will end up being true given the choice of initialization we will choose):</p>
                <script>
                    var render = katex.renderToString("z_k^{[l]} = \\sum_{j=1}^{n^{[l-1]}}w_{kj}^{[l]}a_j^{[l-1]}", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>Looking element-wise at equation (**) now gives:</p>
                <script>
                    var render = katex.renderToString("Var(a_k^{[l]}) = Var(z_k^{[l]}) = Var(\\sum_{j=1}^{n^{[l-1]}}w_{kj}^{[l]}a_j^{[l-1]})", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>A common “math trick” at this point is to extract the summation outside the variance. To do this we must make the following three assumptions:</p>
                <ol>
                    <li>Weights are independent and identically distributed</li>
                    <li>Inputs are independent and identically distributed</li>
                    <li>Weights and inputs are mutually independent</li>
                </ol>
                <p>Thus, now we have:</p>
                <script>
                    var render = katex.renderToString("Var(a_k^{[l]}) = Var(z_k^{[l]}) = Var(\\sum_{j=1}^{n^{[l-1]}}w_{kj}^{[l]}a_j^{[l-1]}) = \\sum_{j=1}^{n^{[l-1]}}Var(w_{kj}^{[l]}a_j^{[l-1]})", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>Another common “math trick” is to convert the variance of a product into product of variances, and we have a formula for it:</p>
                <script>
                    var render = katex.renderToString("Var(XY) = E[X]^2Var(Y) + Var(X)E[Y]^2 + Var(X)Var(Y)", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>Using this formula with <script>document.write(katex.renderToString("X = w_{kj}^{[l]}"))</script>and <script>document.write(katex.renderToString("Y = a_j^{[l-1]}"))</script>, we get:</p>
                <script>
                    var render = katex.renderToString("Var(w_{kj}^{[l]}a_j^{[l-1]}) = E[w_{kj}^{[l]}]^2Var(a_j^{[l-1]}) + Var(w_{kj}^{[l]})E[a_j^{[l-1]}]^2 + Var(w_{kj}^{[l]})Var(a_j^{[l-1]})", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>We’re almost done! A1 leads to <script>document.write(katex.renderToString("E[w_{kj}^{[l]}]^2 = 0"))</script> and A2 leads to <script>document.write(katex.renderToString("E[a_j^{[l-1]}]^2 = 0"))</script>given the initialization scheme we’re using. Thus,</p>
                <script>
                    var render = katex.renderToString("Var(z_k^{[l]}) = \\sum_{j=1}^{n^{[l-1]}}Var(w_{kj}^{[l]})Var(a_j^{[l-1]}) = \\sum_{j=1}^{n^{[l-1]}}Var(W^{[l]})Var(a^{[l-1]}) = n^{[l-1]}Var(W^{[l]})Var(a^{[l-1]})", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>The equality above results from A1 stating that for any entry <script>document.write(katex.renderToString("(i,j)"))</script> of <script>document.write(katex.renderToString("W^{[l]}"))</script>:</p>
                <script>
                    var render = katex.renderToString("Var(w_{kj}^{[l]}) = Var(w_{11}^{[l]}) = Var(w_{12}^{[l]})=\\dots = Var(W^{[l]})", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>Similarly A2 leads to :</p>
                <script>
                    var render = katex.renderToString("Var(a_j^{[l-1]}) = Var(a_1^{[l-1]}) = Var(a_2^{[l-1]})=\\dots = Var(a^{[l-1]})", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>With the same idea:</p>
                <script>
                    var render = katex.renderToString("Var(z^{[l]}) = Var(z_k^{[l]})", {displayMode: true});
                    document.writeln(render);
                </script>
                <p>Wrapping up everything, we have:</p>
                <script>
                    var render = katex.renderToString("Var(a^{[l]}) = Var(z^{[l]}) = n^{[l-1]}Var(W^{[l]})Var(a^{[l-1]})", {displayMode: true});
                    document.writeln(render);
                </script>
            </div>
        </div>
    </div>
    <div class="footer">
        <div class="container">
            <div class="column-2-8 column-align">
                <h4 class="reference">Authors</h4>
            </div>
            <div class="column-6-8 column-align">
                <ol class="reference">
                    <li>Writing - Kian Katanforoosh</li>
                    <li>Visualizations - Daniel Kunin</li>
                    <li>Design - Jingru Guo</li>
                </ol>
            </div>
            <div class="column-2-8 column-align">
                <h4 class="reference">Acknowledgments</h4>
            </div>
            <div class="column-6-8 column-align">
                <ol class="reference">
                    <li>Daniel Smilkov's and Shan Carter's Neural Network <a href="https://playground.tensorflow.org/">playground</a>.</li>
                    <li>Andrej Karpathy's <a href="https://cs.stanford.edu/~karpathy/mnistjs/">visualization</a> of MNIST.</li>
                    <li>Distil Pub's <a href="https://distill.pub/">design</a>.</li>
                </ol>
            </div>
            <div class="column-2-8 column-align">
                <h4 class="reference">Footnotes</h4>
            </div>
            <div class="column-6-8 column-align">
                <ol class="reference">
                    <li>If X is a vector or a matrix, what does Var(X) concretely mean? Take this example, X is the input grayscale image of a neural network. This image comes from a distribution called “real world images”. If you select randomly 100000 images from this distribution and each time, you collect the value of a random pixel  in the middle of the image. What would the distribution of  look like. Maybe something like this:
                    <img src="img/normal.png">
                    All in all, Var(X) means the variance of any entry of the matrix (or vector) X (it is fair to assume they’re all distributed identically.)</li>
                    <li>Concretely it means we pick every weight randomly and independently from a normal distribution centered in  and with variance .</li>
                    <li>We assume that  (resp. ) is initialized with small values (resp. with zeros). Hence,  is small and we are in the linear regime of </li>
                    <li>A1 will end up being true given our initialization scheme (we pick weights randomly according to a normal distribution centered in 0). A2 is not always true. For instance in images, inputs are pixel values, and pixel values in the same region are highly correlated with each other. On average, it’s more likely that a green pixel is surrounded by green pixels than by any other pixel color, because this pixel might be representing a grass field, or a green object. Although it’s not always true, we assume that inputs are distributed identically (let’s say from a normal distribution centered in 0.) A3 this is most of the time true at initialization, given that our initialization scheme makes our weights independent and identically distributed (i.i.d.) The fact that these assumptions are not always true doesn’t end up being a problem. The theory generalizes well to practical application.</li>
                </ol>
            </div>
        </div>
    </div>
</body>

<!-- PLAYGROUND -->
<!-- Main CSS -->
<link rel="stylesheet" href="css/playground.css">
<!-- Data Generattion Code -->
<script src="js/playground/data.js"></script>
<!-- Neural Network Code -->
<script src="js/playground/nn.js"></script>
<!-- Visualization Code -->
<script src="js/playground/viz.js"></script>


<!-- MNIST -->
<!-- Main CSS -->
<link rel="stylesheet" href="css/mnist.css">
<!-- Neural Network Code -->
<script src="js/mnist/nn.js"></script>
<!-- Data Zip Reader -->
<script src="js/mnist/zip/zip.js"></script>
<!-- Data Generattion Code -->
<script src="js/mnist/data.js"></script>
<!-- Visualization Code -->
<script src="js/mnist/viz.js"></script>

<!--Scroll To-->
<script src="js/scroll-to.js"></script>
