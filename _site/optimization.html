
<!DOCTYPE html>
<html>
    <head>
        
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="author" content="Jingru Guo">

<title>Optimizing neural networks</title>




<!-- Fonts -->
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
<link href="https://fonts.googleapis.com/css?family=Assistant:300,400,600,700" rel="stylesheet">
<!-- Load jquery -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<!--Favicon-->
<link rel="shortcut icon" type="image/png" href="/assets/images/layout/favicon.png"/>





<!-- Article CSS -->
<link rel="stylesheet" href="/assets/css/article.css">

<!-- Load D3 -->
<script src="https://d3js.org/d3.v5.min.js"></script>

<!-- Load Tensorflow -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.3/dist/tf.min.js"></script>

<!-- Load Katex -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body, {delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false}
    ]});">
</script>

<!-- Load GreenSock and Snap SVG -->
<script src="/assets/js/TweenMax.min.js"></script>
<script src="/assets/js/Draggable.min.js"></script>
<script src="/assets/js/DrawSVGPlugin.min.js"></script>
<script src="/assets/js/MorphSVGPlugin.min.js"></script>
<script src="/assets/js/ThrowPropsPlugin.min.js"></script>
<script src="/assets/js/snap.svg-min.js"></script>

<!-- Load JS -->
<script src= "/assets/js/cppn.js"></script>
<script src= "/assets/js/d3.tip.js"></script>
<script src= "/assets/js/tool.js"></script>
<script src="https://d3js.org/d3-contour.v1.min.js"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/d3-legend/2.25.6/d3-legend.min.js"></script>


<!-- Load Highlight -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/monokai-sublime.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


    
     

    </head>


  <body>
    <header class="header"> 
    <div class="header-wrapper">
        <ul>
            <li>
                <a href="https://www.deeplearning.ai/">
                    <img src="/assets/images/layout/deeplearning.png">
                </a>
            </li>
            <li> <a href="/" class="backToBlog">AI Notes</a></li>
           
              
                <li class="header-nav-article"><a href="/initialization">Initialization</a></li>
              
           
              
                <li class="header-nav-article"><a href="/optimization">Optimization</a></li>
              
           
              
                <li class="header-nav-article"><a href="/regularization">Regularization</a></li>
              
           
        </ul>
    </div>
</header> 

     <div class="main">
        <div class="container article-banner" >
        	
            <div class="article-banner-content" id="vis-background">
    <div id="cppn-overlay"></div>  
</div>
<div>
    <a class="cppn-control-toggle">
      <div class="cppn-control">
        <i class="fas fa-sliders-h"></i>
      </div>
    </a>
    <div class="cppn-control" style="display: none">
      <a class="cppn-control-toggle fa fa-times"></a>
      <p>Network Depth:</p>
      <label class="radio-container">
          Shallow
          <input type="radio" name="depth" value="3" checked/>
          <span class="checkmark"></span>
      </label>
      <label class="radio-container">
          Deep
          <input type="radio" name="depth" value="4" />
          <span class="checkmark"></span>
      </label>
      <p>Layer Complexity:</p>
      <label class="radio-container">
          Simple
          <input type="radio" name="complexity" value="20" checked/>
          <span class="checkmark"></span>
      </label>
      <label class="radio-container">
          Complex
          <input type="radio" name="complexity" value="25" />
          <span class="checkmark"></span>
      </label>
      <p>Nonlinearity:</p>
      <select id="activation" class="select-containter">
          <option value="sin" selected>Sine</option>
          <option value="cos">Cosine</option>
          <option value="tanh">Tanh</option>
          <option value="linear">Linear</option>
          <option value="step">Step</option>
          <option value="relu">Relu</option>
          <option value="leakyRelu">Leaky Relu</option>
      </select>
    </div>
</div>
<script type="text/javascript">

    $(".cppn-control-toggle").click(function() {
      $(".cppn-control").toggle();
    })

    var c1 = "120 184 66".split(" "),
        c2 = "45 155 106".split(" ");
        c3 = "0 125 117".split(" ");

    var cppn = cppnSetup([c1, c2, c3]),
        layers = 3,
        unit = 20
        activation = "sin";

    $("input[name='depth']").on("change", function () {
      layers = parseInt($(this).val());
      cppn.update(architecture(layers, unit), activation)
    });

    $("input[name='complexity']").on("change", function () {
      unit = parseInt($(this).val());
      cppn.update(architecture(layers, unit), activation)
    });

    $("#activation").on("change", function() {
      activation = $(this).val();
      cppn.update(architecture(layers, unit), activation)
    });


    function architecture(layers, units) {
      var arr = [5];
      for (var i = 0; i < layers; i++) {
        arr.push(units);
      }
      arr.push(3);
      return arr;
    }
</script>
            
            <div class="banner-title" >
                <h1>Optimizing neural networks</h1>
                <p>Training a machine learning model is a matter of closing the gap between the model's predictions and reality. But optimizing the model isn't so straightforward. Through interactive visualizations, we'll help you develop your intuition for setting up and solving the optimization problem.</p>
            </div>
            
        </div>


        <div class="tableOfContent">
        		<p>TABLE OF CONTENT</p>
	        	<ul id="toc">
	        		
	        			<li><span> I </span> <a href="#I">Setting up the optimization problem</a></li>
	                
	        			<li><span> II </span> <a href="#II">Running the optimization process</a></li>
	                
	        	</ul>

        </div>

        <section class="article-content">
        		<p>In machine learning, you start by defining a task and a model. The model consists of an architecture and parameters. For a given architecture, the values of the parameters determine how accurately the model performs the task. 
But how do you find good values? By defining a loss function that evaluates how well the model performs. The goal is to optimize the loss and thereby to find parameter values that match predictions with reality. This is the essence of training.</p>

<h1 id="I">I Setting up the optimization problem</h1>

<p>The loss function will be different in different tasks depending on the output desired. How you define it has a major influence on how the model will train and perform. Let’s consider two examples:</p>

<h3 id="example-1-house-price-prediction">Example 1: House price prediction</h3>

<p>Say your task is to predict the price of houses y \in \mathbb{R}y∈R based on features such as floor area, number of bedrooms, and ceiling height. The loss function can be summarized by the sentence:</p>

<blockquote>
  <p>Given a set of house features, the square of the difference between your prediction and the actual price should be as small as possible.</p>
</blockquote>

<p>You define the loss function as</p>

<div class="kdmath">$$
\mathcal{L} = ||y-\hat{y}||_2^2L=∣∣y−y^​∣∣_
$$</div>

<p>where \hat{y}y^​ is your predicted price and yy is the actual price, also known as ground truth.</p>

<h3 id="example-2-object-localization">Example 2: Object localization</h3>

<p>Let’s consider a more complex example. Say your task is to localize the car in a set of images that contain one. The loss function should frame the following sentence in mathematical terms:</p>

<blockquote>
  <p>Given an image containing one car, predict a bounding box (bbox) that surrounds the vehicle. The predicted box should match the size and position of the actual car as closely as possible.</p>
</blockquote>

<p>In mathematical terms, a possible loss function \mathcal{L}L (Redmon et al., 2016) is:</p>

<p>\mathcal{L} = \underbrace{(x - \hat{x})^2 + (y - \hat{y})^2}<em>{\text{BBox Center}} + \underbrace{(w - \hat{w})^2 + (h - \hat{h})^2}</em>{\text{BBox Width/Height}}L=(+(</p>

<p>This <span class="sidenote">loss function</span> depends on:</p>

<ol>
  <li>The model’s prediction which, in turn, depends on the parameter values  (weights and biases) as well as the input (in this case, images)</li>
  <li>The ground truth corresponding to the input (labels; in this case, bounding boxes)</li>
</ol>

<h3 id="visualizing-the-loss-function">Visualizing the loss function</h3>

<p>For a given input batch along with the corresponding ground truths, the loss function has a landscape that depends on the parameters of the network.</p>

<p>It is difficult to visualize this landscape, if there are more than two parameters. However, the landscape does exist, and our goal is to find the point where the loss function’s value is minimal.</p>

<p>Updating the parameter values will move the value either closer to or farther from the target minimum point.</p>

<h3 id="the-relationship-between-the-model-and-the-loss-function">The relationship between the model and the loss function</h3>

<p>It is important to distinguish between the function f that will perform the task (the model) and the function \mathcal{L} you are optimizing (the loss function).</p>

<ol>
  <li>The model is an architecture and a set of parameters that approximates a <span class="sidenote">real function</span>that performs the task. Optimized parameter values will enable the model to perform the task with relative accuracy.</li>
  <li>The loss function quantifies how accurately the model has performed on given data set. Its value depends on the model’s parameter values.</li>
</ol>

<p>At this point, good parameter values are unknown. However, you have a formula for the loss function. Optimize that on your dataset, and theoretically you will find good parameter values. The way to do this is to feed a training data set into the model and adjust the parameters iteratively to make the loss function as small as possible.</p>

<p>In summary, the way you define the loss function will dictate the performance of your model on the task at hand. The diagram below illustrates the process of finding a model that performs well.</p>

<h1 id="I">II Running the optimization process</h1>

<p>In this section, we assume that you have chosen a task, a data set, and a loss function. You will minimize the loss on the data set to find good parameter values.</p>

<h3 id="using-gradient-descent">Using gradient descent</h3>

<p>First, you must initialize the parameter values so you have a starting point for optimization. Then, you will adjust the parameter values using gradient descent to reduce the value of the loss function.</p>

<p>Gradient descent is an iterative optimization algorithm that finds the minimum of a function. In machine learning, that means minimizing the loss. At every iteration, parameter values are adjusted according to the opposite direction of the gradient of the loss; that is, in the direction that reduces the loss.</p>

<p>The formula to remember is:</p>

<div class="kdmath">$$
W = W - \alpha \frac{\partial \mathcal{L}}{\partial W}
$$</div>

<p>Where:</p>

<ol>
  <li>$W$ denotes the parameters</li>
  <li>$\frac{\partial \mathcal{L}}{\partial W}$ is a gradient indicating the direction to push the value of $W$ in order to decrease $\mathcal{L}$.</li>
  <li>$\alpha$ is the learning rate which you can tune to decide how much you want to adjust the value of $W$.</li>
</ol>

<p>You can learn more about gradient-based optimization algorithms in the Deep Learning Specialization. This topic is covered in Course 1, Week 2 (Logistic Regression as a Neural Network) and Course 2, Week 2 (Optimization Algorithms).</p>

<h3 id="adjusting-gradient-descent-hyperparameters">Adjusting gradient descent hyperparameters</h3>

<p>To use gradient descent, you must choose values for hyperparameters such as learning rate and batch size. These values will influence the optimization, so it’s important to set them appropriately.</p>

<p>In the visualization below, you can play with the starting point of initialization, learning rate, and batch size. With these hyperparameters, you will fit a linear model <span class="kdmath">$\hat{y} = wa x + b$</span> (for the sake of simplicity) on a set of 300 data points using gradient descent.</p>

<p>Here are some questions to consider as you explore the visualization:</p>

<p>What is the impact of the training set size?</p>

<p>What is the impact of the learning rate on the optimization?</p>

<p>What is the impact of the batch size on the optimization?</p>

<p>Why does the loss landscape look like this?</p>

<p>Why do the model parameters converge to values different than those of the ground-truth slope and intercept?</p>

<div class="visualization hide-backToTop" id="regression">
    <div class="visualization-column-1">
        <h3>1. Generate your dataset</h3>
        <p>Select a training set size.</p>
        <label class="radio-container">Small
            <input type="radio" value="20" name="regression_tsize" />
            <span class="checkmark"></span>
        </label>
        <label class="radio-container">Medium
            <input type="radio" value="300" name="regression_tsize" checked="" />
            <span class="checkmark"></span>
        </label>
        <label class="radio-container">Large
            <input type="radio" value="800" name="regression_tsize" />
            <span class="checkmark"></span>
        </label>
        <p>A training set of the chosen size will be sampled with noise from a <span class="blue">"ground truth"</span> line. This line is the target line for your <span class="red">network function</span> defined by <script>document.write(katex.renderToString('\\hat{y} = wx + b'))</script>.</p>
        <div id="regression_plot" style="margin-top:1em;border: 1px solid rgba(0,0,0,0.2);"></div>
        <button class="button-transport" id="generate">
           Generate a new <span class="blue">"ground truth"</span> line
        </button>
    </div>
    <div class="visualization-column-2">
        <h3>2. Observe the loss landscape and initialize parameters</h3>
        <p>The loss function is the L2 loss defined as <script>document.write(katex.renderToString('\\mathcal{L}(y, \\hat{y}) = ||y - \\hat{y}||_2^2'))</script>. The <span class="blue">blue dot</span> indicates the value of the loss function at the "ground truth" slope and intercept. The <span class="red">red dot</span> indicates the value of the loss function at a chosen initialization of the slope and intercept. Drag and drop the red dot to change the initialization.</p>
        <div id="regression_landscape"></div>
    </div>
    <div class="visualization-column-1">
        <h3>3. Optimize your loss function</h3>
        <p>You can now iteratively update your parameters to minimize the loss. Select a learning rate.</p>
        <label class="radio-container">Small
            <input type="radio" value="0.0001" name="regression_lrate" />
            <span class="checkmark"></span>
        </label>
        <label class="radio-container">Medium
            <input type="radio" value="0.01" name="regression_lrate" checked="" />
            <span class="checkmark"></span>
        </label>
        <label class="radio-container">Large
            <input type="radio" value="0.1" name="regression_lrate" />
            <span class="checkmark"></span>
        </label>
        <p>Select a batch size to use.</p>
        <label class="radio-container">Stochastic
            <input type="radio" value="1" name="regression_bsize" checked="" />
            <span class="checkmark"></span>
        </label>
        <label class="radio-container">Mini-batch
            <input type="radio" value="30" name="regression_bsize" />
            <span class="checkmark"></span>
        </label>
        <label class="radio-container">Full-batch
            <input type="radio" value="300" name="regression_bsize" />
            <span class="checkmark"></span>
        </label>
        <p>Train your <span class="red">network function</span>.</p>
        <button class="button-transport" id="regression_reset" title="reset">
            <img src="/assets/images/layout/reset.png" />
        </button>
        <button class="button-transport" id="regression_train" title="start">
            <img src="/assets/images/layout/play.png" />
        </button>
        <button class="button-transport hidden" id="regression_stop" title="stop">
            <img src="/assets/images/layout/pause.png" />
        </button>
        <div id="regression_loss"></div>
    </div>
</div>

<p>Note that the loss $\mathcal{L}$ takes as input a single example, so minimizing it doesn’t guarantee better model parameters for other examples. It is common to minimize the average of the loss computed over a batch of examples; for instance, $\mathcal{J} = \frac{1}{m_b} \sum_{i=1}^{m_b} \mathcal{L}^{(i)}$. We call this function the cost, and reducing it leads to a more accurate parameter-update direction to minimize training error. $m_b$  is called the batch size. This is a key <span class="sidenote">hyperparameter</span> to tune.</p>

<p>Here are some takeaways from the visualization:</p>

<h3 id="initialization">Initialization</h3>

<p>A good initialization can accelerate optimization and enable it to converge to the minimum or, if there are several minima, the best one. To learn more about initialization, read our AI Note on initializing neural networks.</p>

<h3 id="learning-rate">Learning rate</h3>

<p>The learning rate influences the optimization’s convergence. It also counterbalances the influence of the loss function’s curvature. According to the gradient descent formula above, the direction and magnitude of the parameter update is given by the learning rate multiplied by the slope of the loss function at a certain point $W$. Specifically: $\alpha \frac{\partial \mathcal{L}}{\partial W}$.</p>

<p>If the learning rate is too small, updates are small and optimization is slow, especially if the loss curvature is low. Also, you’re likely to settle into an <span class="sidenote">inappropriate local minimum<span>.</span></span></p>

<p>If the learning rate is too large, updates will be large and the optimization is likely to diverge, especially if the loss curvature is high.</p>

<p>If the learning rate is good, updates are appropriate and the optimization should converge.</p>

<p>Play with the visualization below to understand the influence of the learning rate and the loss curvature on the convergence of your algorithm.</p>

<div class="visualization hide-backToTop">
  <div class="visualization-column-2">
    <svg id="loss" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 565 403">
        <rect id="frame" width="565" height="403" fill="none" />
        <g id="loss_graph" data-name="loss graph">
          <g id="axes">
            <line id="bottomLine" x1="61.48" y1="364.65" x2="523.48" y2="364.65" fill="none" stroke="#aaa" stroke-miterlimit="10" stroke-width="3" />
            <line id="vert" x1="61.48" y1="364.65" x2="61.48" y2="17.55" fill="none" stroke="#aaa" stroke-miterlimit="10" stroke-width="3" />
            <polygon id="left_arr" data-name="left arr" points="521.68 359.15 521.68 369.35 532.08 364.65 521.68 359.15" fill="#aaa" />
            <polygon id="up_arr" data-name="up arr" points="56.38 19.05 66.68 19.05 61.88 8.65 56.38 19.05" fill="#aaa" />
          </g>
          <line id="errorLine" x1="3" y1="8.65" x2="3" y2="8.65" fill="none" stroke="#ff1e00" stroke-miterlimit="10" stroke-width="3" />
          <path id="flat_curve" data-name="flat curve" d="M93.22,59.88S188.5,299.22,311.5,299.22,529.85,59.88,529.85,59.88" fill="none" stroke="#2665ba" stroke-linecap="round" stroke-miterlimit="10" stroke-width="3" />
          <path id="steep_curve" data-name="steep curve" d="M166.62,59.88S229.9,299.22,311.5,299.22s145-239.34,145-239.34" fill="none" stroke="#2665ba" stroke-linecap="round" stroke-miterlimit="10" stroke-width="3" />
          <line id="deriv" x1="523.48" y1="299.22" x2="99.59" y2="299.22" fill="none" stroke="red" stroke-miterlimit="10" stroke-width="3" />
          <line id="ph_deriv" data-name="ph deriv" x1="523.48" y1="299.22" x2="99.59" y2="299.22" fill="none" stroke="#ffa600" stroke-linecap="round" stroke-miterlimit="10" stroke-width="3" />
          <line id="learnLine" x1="3" y1="8.65" x2="3" y2="8.65" fill="none" stroke="#d45087" stroke-linecap="round" stroke-miterlimit="10" stroke-width="4" />
          <circle id="cloneDot" cx="311.54" cy="299.22" r="6.66" fill="#ff7c43" />
          <circle id="phDot" cx="311.54" cy="299.22" r="6.66" fill="none" />
          <circle id="centerDot" cx="311.54" cy="299.22" r="6.66" fill="#5ea0fa" />
          <text transform="translate(10.35 19.05)" font-size="15" fill="#878787" font-family="Roboto-Regular, Roboto" style="isolation: isolate">Loss</text>
          <g>
            <text transform="translate(483.95 394.12)" font-size="15" fill="#878787" font-family="Roboto-Regular, Roboto" letter-spacing="-0.01em" style="isolation: isolate">P</text>
            <text transform="translate(493.33 394.12)" font-size="15" fill="#878787" font-family="Roboto-Regular, Roboto" style="isolation: isolate">a</text>
            <text transform="translate(501.49 394.12)" font-size="15" fill="#878787" font-family="Roboto-Regular, Roboto" letter-spacing="-0.02em" style="isolation: isolate">r</text>
            <text transform="translate(506.27 394.12)" font-size="15" fill="#878787" font-family="Roboto-Regular, Roboto" style="isolation: isolate">ameters</text>
          </g>
        </g>
    </svg>
  </div>
  <div class="visualization-column-1">
    <p id="iter_text">Iteration: <span id="curvature_ittr">0</span></p>
    <p>Loss function curvature:</p>
    <label id="lo_toggle" class="radio-container">Low
        <input type="radio" value="20" name="curvature_lrate" checked="" />
        <span class="checkmark"></span>
    </label>
    <label id="hi_toggle" class="radio-container">High
        <input type="radio" value="300" name="curvature_lrate" />
        <span class="checkmark"></span>
    </label>
    <p>Learning rate:</p>
    <label id="sm_toggle" class="radio-container">Small
        <input type="radio" value="800" name="curvature_crate" />
        <span class="checkmark"></span>
    </label>
    <label id="big_toggle" class="radio-container">Large
        <input type="radio" value="800" name="curvature_crate" checked="" />
        <span class="checkmark"></span>
    </label>
    <div>
    <button class="button-transport" id="replay" title="reset">
        <img src="/assets/images/layout/reset.png" /> Reset
    </button>
    <br />
    <img src="/assets/images/article/optimization/approx_error.png" style="width:50%" />
    </div>
  </div>
</div>

<p>Takeaways from the visualization:</p>

<p>The choice of learning rate depends on the curvature of the loss function.</p>

<p>Gradient descent makes a linear approximation of the loss function at a given point. Then it moves downhill along the approximation of the loss function.</p>

<p>If the loss is highly curved, the larger the learning rate (step size), the larger the error of the <span class="sidenote">gradient approximation<span>. The approximation tends to overshoot.</span></span></p>

<p>Taking small steps reduces the gradient approximation error.</p>

<p>It is common to start with a large learning rate and decay it during training. Learning rates depend on the application; values between 0.1 and 1 are usually considered large. Choosing the right decay (how often? by how much?) is non-trivial. An excessively aggressive decay schedule slows progress toward the optimum, while a slow-paced decay schedule leads to chaotic updates with small improvements.</p>

<p>In fact, nobody knows the right decay schedule. However, adaptive learning-rate algorithms such as Momentum Adam and RMSprop help adjust the learning rate during the optimization process. We’ll explain those algorithms below.</p>

<h3 id="batch-size">Batch size</h3>

<p>Batch size is the number of data points used to train a model in each iteration. Typical small batches are 32, 64, 128, 256, 512, while large batches can be thousands of examples.</p>

<p>Choosing the right batch size is crucial to ensure convergence of the loss function and parameter values, and to the generalization of your model. Research2 has considered how to make the choice, but there is no consensus. In practice, you can use a <span class="sidenote">hyperparameter search<span>.</span></span></p>

<p>Research into batch size has revealed the following principles:</p>

<p>Batch size determines the frequency of updates. The smaller the batches, the more — though quicker — the updates.</p>

<p>The larger the batch size, the more accurate the gradient of the loss will be with respect to the parameters. That is, the direction of the update is most likely going down the local 
slope of the loss landscape.</p>

<p>The largest batch size that fits into GPU memory leads to efficient parallelization and usually accelerates training.</p>

<p>However, in practice, large batch sizes can hurt the model’s ability to generalize.</p>

<p>In choosing batch size, there’s a balance to be struck depending on the available computational hardware and the task you’re trying to achieve. Recall that the input batch is an input to the cost function. Large batch size typically leads to sharper cost function surfaces than a small batch size, as Keskar et al. find in their paper, “On large-batch training for deep learning: generalization gap and sharp minima.”</p>

<p>Here’s a figure comparing a flat and a sharp minimum. Flat cost surfaces (and thus small batch sizes) are preferred because they lead to good generalization without requiring high precision.</p>

<p>In practice, hyperparameter search can help you find batch size and learning rate. These hyperparameters are two routes to the same outcome, according to Smith, Kindermans et al. in Don’t Decay the Learning Rate, Increase the Batch Size. They argue that the benefits of decaying the learning rate can be achieved by increasing batch size during training. So if you change batch size, you may also need to change learning rate. Efficient use of vast batch sizes notably reduces the number of parameter updates required to train a model.</p>

<h3 id="iterative-update">Iterative update</h3>

<p>Now that you have a starting point, a learning rate, and a batch size, it’s time to update the parameters iteratively to move toward the cost function’s minimum.</p>

<p>The optimization algorithm is also a core choice. You can play with various optimizers in the visualization below. That will help you build an intuitive sense of the pros and cons of each.</p>

<p>In this visualization, your goal is to play with hyperparameters to find parameter values that minimize a loss function. You can choose the loss function and starting point of the optimization. Although there’s no explicit model, you can assume that finding the minimum of the loss function is equivalent to finding the best model for your task. For the sake of simplicity, the model only has two parameters and the batch batch size is always 1.</p>

<div class="visualization hide-backToTop" id="landscape">
    <div class="visualization-column-2">
        <p>In this visualization, you will choose the loss function and the starting point of your optimization. Although there's no explicit network function, you can assume that finding the minimum of your loss function is equivalent to finding the best network function for your task.</p>

        <h3>1. Choose a loss landscape</h3>
        <p>Select an <a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization">artificial landscape</a> $\mathcal{L}(w_1,w_2)$.</p>
        <div class="lossFunctions">
            <label>
              <input type="radio" name="loss" value="himmelblaus" checked="" />
              <img src="/assets/images/article/optimization/loss/himmelblaus.png" />
            </label>
            <label>
              <input type="radio" name="loss" value="styblinskiTang" />
              <img src="/assets/images/article/optimization/loss/styblinskiTang.png" />
            </label>
            <label>
              <input type="radio" name="loss" value="rosenbrock" />
              <img src="/assets/images/article/optimization/loss/rosenbrock.png" />
            </label>
            <label>
              <input type="radio" name="loss" value="goldsteinPrice" />
              <img src="/assets/images/article/optimization/loss/goldsteinPrice.png" />
            </label>
        </div>
        <h3>2. Choose initial parameters</h3>
        <p>On the loss landscape graph, drag the <font color="red">red dot</font> to choose the initial parameters values and thus the initial value of the loss.</p>

        <h3>3. Choose an optimizer</h3>
        <p>Select the optimizer and its hyperparameters.</p>


        <table>
          <tr>
            <th>Optimizer</th>
            <th>Learning Rate</th> 
            <th>Learning Rate Decay</th>
          </tr>
          <tr>
            <td>
                <div class="checkbox">
                    <input type="checkbox" name="opt" value="gd" checked="" />
                    <label>Gradient Descent</label>
                </div>
                
            </td>
            <td><input class="gd" type="number" name="lrate" value="0.001" min="0" max="1" step="0.0001" /></td>
            <td><input class="gd" type="number" name="ldecay" value="0" min="0" max="1" step="0.01" /></td>
          </tr>
          <tr>
            <td><input type="checkbox" name="opt" value="momentum" checked="" /> Momentum</td>
            <td><input class="momentum" type="number" name="lrate" value="0.001" min="0" max="1" step="0.0001" /></td>
            <td><input class="momentum" type="number" name="ldecay" value="0" min="0" max="1" step="0.01" /></td>
          </tr>
          <tr>
            <td><input type="checkbox" name="opt" value="rmsprop" checked="" /> RMSprop</td>
            <td><input class="rmsprop" type="number" name="lrate" value="0.001" min="0" max="1" step="0.0001" /></td>
            <td><input class="rmsprop" type="number" name="ldecay" value="0" min="0" max="1" step="0.01" /></td>
          </tr>
          <tr>
            <td><input type="checkbox" name="opt" value="adam" checked="" /> Adam</td>
            <td><input class="adam" type="number" name="lrate" value="0.001" min="0" max="1" step="0.0001" /></td>
            <td><input class="adam" type="number" name="ldecay" value="0" min="0" max="1" step="0.01" /></td>
          </tr>
        </table>
        <p>Optimize your loss function.</p>
        <button class="button-transport" id="reset" title="reset">
            <img src="/assets/images/layout/reset.png" />
        </button>
        <button class="button-transport" id="train" title="start">
            <img src="/assets/images/layout/play.png" />
        </button>
        <button class="button-transport hidden" id="stop" title="stop">
            <img src="/assets/images/layout/pause.png" />
        </button>
    </div>
    <div class="visualization-column-2">
        <p> This 2D plot describes the value of your loss function for different values of the 2 parameters $(w_1,w_2)$. The darker the color, the larger the loss value.</p>
        <div id="landscape_contour"></div>
        <div id="landscape_loss"></div>
    </div>
</div>

<p>The choice of optimizer influences both the speed of convergence and whether it occurs. Several alternatives to the classic gradient descent algorithms have been developed in the past few years and are listed in the table below. (Notation: dW = \frac{\partial \mathcal{L}}{\partial W}dW=∂W∂L​)</p>

<p>Adaptive optimization methods such as Adam or RMSprop perform well in the initial portion of training, but they have been found to generalize poorly at later stages  compared to Stochastic Gradient Descent. In Improving Generalization Performance by Switching from Adam to SGD, Keskar et al. investigate a hybrid strategy that begins training with an adaptive method and switches to SGD.</p>

<p>You can find more information about these optimizers in the Deep Learning Specialization Course 2: Improving your Deep Neural Network, Week 2 (Optimization) on Coursera.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Exploring optimization methods and hyperparameter values can help you build intuition for optimizing networks for your own tasks. During hyperparameter search, it’s important to understand intuitively the optimization’s sensitivity to learning rate, batch size, optimizer, and so on. That intuitive understanding, combined with the right method (random search or Bayesian optimization), will help you find the right model.</p>


                
                    <div class="sidenote-body">
                        <p class="caption">By definition, this function L has a low value when the model performs well on the task.</p>
                    </div>
                
                    <div class="sidenote-body">
                        <p class="caption">Do you know the mathematical formula that allows a neural network to detect cats in images? Probably not. But using data you can find a function that performs this task. It turns out that a convolutional architecture with the right parameters defines a function that can perform this task well.</p>
                    </div>
                
                    <div class="sidenote-body">
                        <p class="caption">While model parameters are derived during training, hyperparameters are values set before training starts. Hyperperameters include batch size and learning rate.</p>
                    </div>
                
                    <div class="sidenote-body">
                        <p class="caption">{"We use the term inappropriate local minimum because, in optimizing a machine learning model, the optimization is often non-convex and unlikely to converge to the global minimum. <!-- NOTE"=>"MISSING \"ONELINE\" and \"NONSTATIONARY SETTINGS\" HIGHLIGHTED IN OPTIMIZER TABLE -->"}</p>
                    </div>
                
                    <div class="sidenote-body">
                        <p class="caption">Gradient descent makes a linear approximation of the loss function in a given point. It then moves downhill along the approximation of the loss function.</p>
                    </div>
                
                    <div class="sidenote-body">
                        <p class="caption">{"For more information on hyperparameter tuning, see the Deep Learning Specialization Course 2"=>"Improving Neural Networks, Week 3 (Hyperparameter tuning, Batch Normalization and Programming Frameworks)."}</p>
                    </div>
                
        </section>

    </div>




    <div class="foot-note">

            <div class="foot-note-header">
                <h4 class="reference">Authors</h4>
            </div>
            <div class="foot-note-content">
                <ol class="reference ">
                    
                    <li><a href="https://twitter.com/kiankatan">Kian Katanforoosh</a> - Written content and structure.</li>
                    
                    <li><a href="http://daniel-kunin.com">Daniel Kunin</a> - Visualizations (created using <a href="https://d3js.org/">D3.js</a> and <a href="https://js.tensorflow.org/">TensorFlow.js</a>).</li>
                    
                </ol>
            </div>

            <div class="foot-note-header">
                <h4 class="reference">Acknowledgments</h4>
            </div>
            <div class="foot-note-content">
                <ol class="reference">
                    
                    <li>The template for the article was designed by <a href="https://www.jingru-guo.com/">Jingru Guo</a> and inspired by <a href="https://distill.pub/">Distill</a>.</li>
                    
                    <li>The loss landscape visualization adapted code from Mike Bostock's <a href="https://bl.ocks.org/mbostock/f48ff9c1af4d637c9a518727f5fdfef5">visualization</a> of the Goldstein-Price function.</li>
                    
                    <li>The banner visualization adapted code from deeplearn.js's implementation of a <a href="https://en.wikipedia.org/wiki/Compositional_pattern-producing_network">CPPN</a>.</li>
                    
                </ol>
            </div>

            <div class="foot-note-header">
                <h4 class="reference">Footnotes</h4>
            </div>
            <div class="foot-note-content">
                <ol class="reference" id="fn">
                   
                    <li class="footnote-body"><span><!-- footnote text in order --></span></li>
                    
                </ol>
            </div>


            <div class="foot-note-header">
                <h4 class="reference">Reference</h4>
            </div>
            <div class="foot-note-content">
                <p class="reference">To reference this article in an academic context, please cite this work as:</p>
                <p class="citation">Katanforoosh & Kunin, "Optimizing neural networks", deeplearning.ai, 2019.</p>
               
            </div>

</div>

    

<div class="footer-generic hide-backToTop">

    <div class="container">
        <p class="footer-note">
            Contact us at hello@deeplearning.ai</br>
            © Deeplearning.ai 2018</br>
            <a href="https://www.deeplearning.ai/privacy/">PRIVACY POLICY</a> <a href="https://www.deeplearning.ai/terms-of-use/">TERMS OF USE</a>
        </p>


        <div class="social">
                <a href="https://www.facebook.com/deeplearningHQ/"><i class="fab fa-facebook fontAwesomeIcon" ></i></a>
                <a href="https://twitter.com/deeplearningai_"><i class="fab fa-twitter-square fontAwesomeIcon"></i></a>
                <a href="https://www.linkedin.com/company/deeplearningai/"><i class="fab fa-linkedin fontAwesomeIcon"></i></a>
        </div>
    </div>
</div>
   
<div class="backToTop">
    <p>↑ Back to top</p>
</div>


    
      
      <link rel="stylesheet" href="/assets/css/article/optimization/landscape.css" >
      
      <link rel="stylesheet" href="/assets/css/article/optimization/regression.css" >
      
    

    
      
      <script src="/assets/js/article/optimization/curvature.js" type="text/javascript"></script>
      
      <script src="/assets/js/article/optimization/regression/loss.js" type="text/javascript"></script>
      
      <script src="/assets/js/article/optimization/regression/optimizer.js" type="text/javascript"></script>
      
      <script src="/assets/js/article/optimization/regression/line.js" type="text/javascript"></script>
      
      <script src="/assets/js/article/optimization/regression/viz.js" type="text/javascript"></script>
      
      <script src="/assets/js/article/optimization/landscape/point.js" type="text/javascript"></script>
      
      <script src="/assets/js/article/optimization/landscape/loss.js" type="text/javascript"></script>
      
      <script src="/assets/js/article/optimization/landscape/optimizer.js" type="text/javascript"></script>
      
      <script src="/assets/js/article/optimization/landscape/viz.js" type="text/javascript"></script>
      
       
  

  </body>


  
</html>